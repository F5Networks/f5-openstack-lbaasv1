<!--
# Copyright 2014 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
-->
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii" />
  <title>Neutron/LBaaSv1/F5BigIP</title>
  <meta name="author" content="John Gruber" />
  <link href="data:image/x-icon;base64,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"
  rel="icon" type="image/x-icon" />
  <style type="text/css">
    /*<![CDATA[*/

    body {
      font-family: dejaSansMono;
      src: url('./images/DejaVuSansMono.ttf');
      margin-left: 20px;
      margin-right: 20px;
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      font-size: 14px;
      line-height: 20px;
      color: #333333;
      background-color: #ffffff;
    }

    a {
      color: #0088cc;
      text-decoration: none;
    }

    a:hover,
    a:focus {
      color: #005580;
      text-decoration: underline;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 10px 0;
      font-family: inherit;
      font-weight: bold;
      line-height: 20px;
      color: inherit;
      text-rendering: optimizelegibility;
    }

    h1 small,
    h2 small,
    h3 small,
    h4 small,
    h5 small,
    h6 small {
      font-weight: normal;
      line-height: 1;
      color: #999999;
    }

    h1,
    h2,
    h3 {
      line-height: 40px;
    }

    h1 {
      font-size: 38.5px;
    }

    h2 {
      font-size: 31.5px;
    }

    h3 {
      font-size: 24.5px;
    }

    h4 {
      font-size: 17.5px;
    }

    h5 {
      font-size: 14px;
    }

    h6 {
      font-size: 11.9px;
    }

    h1 small {
      font-size: 24.5px;
    }

    h2 small {
      font-size: 17.5px;
    }

    h3 small {
      font-size: 14px;
    }

    h4 small {
      font-size: 14px;
    }

    pre {
      font-family: dejaSansMono, monospace;
      font-size: 12px;
    }

    ul,
    ol {
      padding: 0;
      margin: 0 0 10px 25px;
    }

    ul ul,
    ul ol,
    ol ol,
    ol ul {
      margin-bottom: 0;
    }

    li {
      line-height: 20px;
    }

    hr {
      margin: 20px 0;
      border: 0;
      border-top: 1px solid #eeeeee;
      border-bottom: 1px solid #ffffff;
    }

    table {
      max-width: 100%;
      background-color: transparent;
      border-collapse: collapse;
      border-spacing: 0;
    }

    table th,
    table td {
      padding: 8px;
      line-height: 20px;
      text-align: left;
      vertical-align: top;
      border-top: 1px solid #dddddd;
    }

    table th {
      font-weight: bold;
      white-space: nowrap;
    }

    table tbody {
      border-top: 2px solid #dddddd;
    }

    .command_text {
      font-family: Courier;
      font-weight: bold;
      font-size: small;
    }

    .firstHeading {
      font-style: italic;
    }

    .mw-headline {
      color: #c01010;
    }
    /*]]>*/
  </style>
  <!-- Date: 2014-03-11 -->
</head>

<body>
  <div id='bodyContent'>
    <h1 id='firstHeading' class='firstHeading'>
Neutron LBaaSv1 - Version 1.0.10 Release
</h1>
    <ul>
      <li>
        <a href='#Supported_Neutron_Releases'>Supported Neutron Releases</a>
      </li>
      <li>
        <a href='#Neutron Networking Prerequisites'>Neutron Networking Prerequisites</a>
      </li>
      <li>
        <a href='#LBaaSv1 Component Distribution'>LBaaSv1 Component Distribution</a>
      </li>
      <li>
        <a href='#General LBaaSv1 Process Architecture'>General LBaaSv1 Process Architecture</a>
      </li>
      <li>
        <a href='#Installing LBaaSv1 Components'>Installing LBaaSv1 Components</a>
      </li>
      <li>
        <a href='#Multiple Controllers and Agent Redundancy'>Multiple Controllers and Agent Redundancy</a>
      </li>
      <li>
        <a href='#iControl Driver Supported TMOS Versions'>iControl Driver Supported TMOS Versions</a>
      </li>
      <li>
        <a href='#Supported Neutron Network Topologies'>Supported Neutron Network Topologies</a>
      </li>
      <li>
        <a href='#OpenStack and TMOS Multinenancy'>OpenStack and TMOS Multinenancy</a>
      </li>
      <li>
        <a href='#Supported TMOS High Availability Modes'>Supported TMOS High Availability Modes</a>
      </li>
      <li>
        <a href='#Agent Configuration Items'>Agent Configuration Items</a>
      </li>
      <li>
        <a href='#Preparing Your TMOS Device for LBaaSv1'>Preparing Your TMOS Device for LBaaSv1</a>
      </li>
      <li>
        <a href='#Troubleshooting_Issues'>Troubleshooting Issues</a>
      </li>
    </ul>
    <h2>

<span class='mw-headline' id='Supported_Neutron_Releases'>
Supported Neutron Releases
</span>
</h2>
    <p>
      The f5 LBaaSv1 service provider drivers and agents support the OpenStack Neutron Icehouse, Juno and Killo releases.
    </p>
    <p>
      There are no alternations made to the community LBaaSv1 database schema, nor any additional database tables generated. The stock community LoadBalancerPlugin class is used as the service plugin. Any discovered issues with the community LoadBalancerPlugin
      class and the associated LBaaSv1 database model will need to be monitored and resolved through the OpenStack Neutron community.
    </p>
    <h2>
    <span class='mw-headline' id='Neutron Networking Prerequisites'>
    Neutron Networking Prerequisites</span>
    </h2>
    <p>
      The f5 LBaaSv1 service provider drivers and agents support two modes of network operations. The Neutron core provider requirements are different for each mode.
    </p>
    <h4>Global Routed Mode</h4>
    <p>
      For global routed mode, the agent will not attempt to manage any L2 networking on TMOS devices. It is assumed that all required L2 networks and associated L3 addressing and routes to cloud resources have been pre-provisioned on the TMOS devices before
      LBaaSv1 objects are created. All traffic to and from the TMOS devices should be routed by external networking devices. All VIPs created by the agents will listen for traffic globally on all provisioned L2 networks.
    </p>
    <p>
      If your TMOS devices are deployed as edge routed devices only, meaning they are typically directly connected to only provider networks, global routed mode is a simplified mode of operations.
    </p>
    <p><img src='./images/global_routed_mode_topology.png' /></p>
    <p>
      In global routed mode, since it is assumed that all routing is pre-provisioned on networking devices, the networking orchestration requirements for Neutron are minimal. Global routed mode should typically function on any Neutron network technology where
      IP packet forwarding to the TMOS devices can be assured.
    </p>
    <p>
      Global routing mode is activated on the agent by setting the configuation parameter <span class='command_text'>f5_global_routed_mode</span> to true.
    </p>
    <h4>L2 Adjacent Mode</h4>
    <p>
      In L2 adjacent mode, the default mode, the LBaaSv1 agent will attempt to provision L2 networks, including VLANs and overlay tunnels, associating TMOS devices with each tenant network contaning a VIP or Member. VIP listening will be restricted to their
      designated Neutron tenant network and L3 addresses associated with Members will be automatically allocated from Neutron subnets.
    </p>
    <p>
      L2 adjacent mode follows the micro-segmentation security model for gateways. Since each TMOS device will be L2 adjacent to all tenant networks for which LBaaSv1 objects are provisioned, the traffic flows will not logically pass through another L3 forwarding
      device, but rather will be isolated to direct L2 communication between the cloud network element and the TMOS devices.
    </p>
    <p><img src='./images/l2_adjacent_mode_topology.png' /></p>
    <p>
      Since the TMOS device associations with many tenant networks are managed by the agents, L2 adjacent mode is a more complex orchestration. The orchestration includes the allocation of L3 addresses from Neutron tenant subnets dynamically for TMOS SelfIPs
      and SNAT translation addresses. These additional L3 addresses will be allocated from the Neutron subnets associated with LBaaSv1 VIPs or Members.
    </p>
    <p>
      For L2 adacent mode deployments which utilize overlay tunnels as tenant networks, Neutron <b>MUST</b> be using the ML2 core plugin. Your neutron server configurations should contain:
    </p>
    <p>
      <span class='command_text'>
core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin
</span>
    </p>
    <p>
      By requiring the use of an ML2 core plugin, Neutron standard GRE and VxLAN tunnel overlay networks, which are compatible with using TMOS as a VTEP (virtual tunnel endpoint), are assured rather then the use of proprietary tunneling or encapsulation schemes
      which are not defined as ML2 network types. ML2 L2 population services, which update the forwarding databases in the VTEPs when changes occur, remove the need any additional control plane orchestration with proprietary network controllers. ML2 provides
      all that is needed to integrate TMOS devices with Neutron.
    </p>
    <p>
      Using the standard community ML2 plugin and type drivers, in addition to the L2 population service will meet the f5 LBaaSv1 plugin requirements to function as a VTEP.
    </p>
    <p>
      When using L2 adjacent mode, Neutron networks associated with LBaaSv1 Vip or Member objects, the Neutron providernet extension <b>MUST</b> also be present. The providernet extension adds attributes to Neutron networks defining appropriate L2 network
      settings required to assure device connectivity. The f5 LBaaSv1 agent uses providernet attributes to provision L2 connectivity on TMOS devices. If the providernet extension data is not present on a Neutron network, proper L2 isolation and tenancy
      can not be provisioned on the TMOS devices.
    </p>
    <p>
      You can determine if your Neutron networks support the providernet extension attributes by showing the details for a Neutron network and checking that the following highlighted attributes exist:
    </p>
    <p>
      <pre>
# neutron net-show Provider-VLAN-62
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 07f92400-4bb6-4ebc-9b5e-eb8ffcd5b34c |
| name                      | Provider-VLAN-62                     |
| <b>provider:network_type</b>     | vlan                                 |
| <b>provider:physical_network</b> | ph-eth3                              |
| <b>provider:segmentation_id</b>  | 62                                   |
| router:external           | False                                |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   | a89aa39e-3a8e-4f2f-9b57-45aa052b87bf |
| tenant_id                 | 3aef8f59a43943359932300f634513b3     |
+---------------------------+--------------------------------------+
</pre>
    </p>
    <h2>
<span class='mw-headline' id='LBaaSv1 Component Distribution'>
LBaaSv1 Component Distribution
</span>
</h2>
    <p>
      The f5 LBaaSv1 solution comprises of three installation packages:
    </p>
    <p>
      <ul>
        <li>f5-bigip-common - Common TMOS API libraries</li>
        <li>f5-oslbaasv1-driver - Neutron Service Provider</li>
        <li>f5-oslbaasv1-agent - Agent process</li>
      </ul>
    </p>
    <p>
      These packages are distributed as Debian deb or Redhat rpm packages. These packages are available at
      <a href='https://devcentral.f5.com/d/openstack-neutron-lbaas-driver-and-agent' target='_blank'>
https://devcentral.f5.com</a>.
    </p>
    <p>
      The f5-oslbaasv1-driver package should be installed on your Neutron controllers where the neutron-server process runs.
    </p>
    <p>
      The f5-bigip-common and f5-oslbaasv1-agent packages should be installed on hosts which will run your agents. This can include your Neutron controller hosts. Be aware that the agent will attempt to access a Neutron configuration file (/etc/neutron/neutron.conf)
      for its messaging settings. The easiest way to assure these settings are correct is to simply copy the /etc/neutron/neutron.conf file from your Neutron controller.
    </p>
    <h2>
<span class='mw-headline' id='General LBaaSv1 Process Architecture'>
General LBaaSv1 Process Architecture
</span>
</h2>
    <p>
      When Neutron LBaaSv1 API calls are issued to your Neutron controller, the community LBaaSv1 plugin will attempt to use either a designated service provider driver, or else the default service provider driver, to provision LBaaSv1 resources.
    </p>
    <p>
      The f5 LBaaSv1 service provider drivers, running within the Neutron controller process(es), utilize Neutron RPC messaging queues to issue provisioning tasks to specific f5 agent processes. Upon starting and successfully communicating with configured TMOS
      device API endpoints, each agent process registers its own specific named queue to receive tasks from one or multiple Neutron controllers.
    </p>
    <p>
      <img src='./images/plugin_agent_architecture.png' width='750px' />
    </p>
    <p>
      The f5 LBaaSv1 agents make callbacks to the f5 LBaaSv1 service provider drivers to query additional Neutron network, port, and subnet information, allocate Neutron objects like fixed IP address for SelfIP and SNAT translation addresses, and report provisioning
      and pool status. These callback requests are placed on an RPC message queue which is processed by all listening f5 LBaaSv1 service provider drivers in a round robin fashion. Since all Neutron controller processes are working transactionally off
      the same backend database, it does not matter which of the available Neutron controller processes handle these callback requests.
    </p>
    <p>
      You <b>MUST</b> have at least one running f5 service provider driver in a Neutron controller, and you <b>MUST</b> at least have one running agent process. Adding additional service provider drivers, one per Neutron controller, will scale out communications
      from agents to Neutron. Adding addition agents, on different hosts, with the same
      <span class='command_text'>environment_prefix</span> and iControl endpoint settings adds scheduled redudancy to the provision process. Neutron LBaaSv1 will always bind pools to specific agents for the life of the pool. The redudancy simply allows
      other agents associated with the same <span class='command_text'>environment_prefix</span> to handle requests if the bound agent is not active. It is expected that the bound agent be brought back online. If an agent is to be deleted, all pools bound
      to it should also be deleted. It is also important that two agents which have different iControl endpoint settings, meaning provisioning different sets of TMOS devices, not be active using the same
      <span class='command_text'>environment_prefix</span> as this will be interpreted by the scheduler as both agents provisioning the same set of TMOS devices.
    </p>
    <p>
      When a LBaaSv1 API interface is invoked, the f5 LBaaSv1 service provider schedules agent tasks based upon an f5 agent's availabilty as updated via the standard Neutron agent status messages. The agent status handling of f5 LBaaSv2 agents is the same agent
      registration and status update mechanism used by the standard Neutron L3, Neutron DHCP, and compute node reference networking agent implementation.
    </p>
    <p>
      You can view all your running Neutron agent processes using the Neutron API agent interfaces. Using the CLI client you can issue the neutron agent-list and neutron agent-show commands.
    </p>
    <p>
      <pre>
# neutron agent-list
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
| id                                   | agent_type         | host                                                 | alive | admin_state_up |
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
| 034bddd0-0ac3-457a-9e2c-ed456dc2ad53 | Open vSwitch agent | sea-osp-cmp-001                                      | :-)   | True           |
| 17e447de-6589-4419-ac83-36ffb7e8b328 | Open vSwitch agent | sea-osp-cmp-002                                      | :-)   | True           |
| 301f6e21-a8f2-4f58-a4a3-38eabc0c2987 | Open vSwitch agent | sea-osp-net-001                                      | :-)   | True           |
| e4638eb3-2c69-4355-9268-2976ec916314 | Open vSwitch agent | sea-osp-net-002                                      | :-)   | True           |
| 5ecd96ab-d01e-4a64-92e8-9cd7caa8f25e | L3 agent           | sea-osp-net-001                                      | :-)   | True           |
| b50b8b21-0d0b-4776-a6ec-eeab61590f10 | DHCP agent         | sea-osp-net-002                                      | :-)   | True           |
| <b>367b0a91-4502-4893-8770-79af8f1f8ef7 | Loadbalancer agent | sea-osp-ctl-001:b739b44f-caab-527f-8ce3-df5527a6981b | :-)   | True</b>           |
| <b>61d2a814-343a-4ba9-a3c0-caa89adc7de3 | Loadbalancer agent | sea-osp-ctl-002:fd0fb108-2748-5d93-bf46-768c18c71fe1 | :-)   | True</b>           |
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
</pre>
    </p>
    <h2>
<span class='mw-headline' id='Installing LBaaSv1 Components'>
Installing LBaaSv1 Components
</span>
</h2>
    <p>
      The most basic installation will include one service provider driver and one agent both installed on a Neutron controller. This is the recommended configuration for testing. Once the operation of the LBaaSv1 orchestration is understood, scale out and
      redudant installations can be added. The Debian and Red Hat package installations assume a single driver and single agent installation. Alterations to the the default installed services to add redudancy and scale out are referenced in following
      sections of this document.
    </p>
    <h3>
Neutron Server Plugin Installation
</h3>
    <p>
      The Neutron LBaaSv1 service provider driver is distributed as a Debian or Red Hat installation package. To install the service provider driver, download the appropriate package to your Neutron controller server host(s) and install it.
    </p>
    <p>
      Debian / Ubuntu example:
    </p>
    <p>
      <span class='command_text'>dpkg -i f5-oslbaasv1-driver_1.0.10-1_all.deb</span>
    </p>
    <p>
      Rad Hat example:
    </p>
    <p>
      <span class='command_text'>rpm -ivh f5-oslbaasv1-driver-1.0.10-1.noarch.el7.rpm</span>
    </p>
    <p>
      Changes to the neutron server configuration file will then need to be made to enable LBaaSv1 services and reference the f5 LBaaSv1 service provider driver. Once the changes in configuration are made, the neutron server will then need to be restarted to
      accept them.
    </p>
    <p>
      There are two configuration items which need to be set to enable the Neutron LBaaSv1 service and the F5 LBaaSv1 service provider driver. The services entry for Neutron must include a reference to the default community LoadBalancerPlugin. The
      <span
      class='command_text'>services</span>
        configuration entry in the default <span class='command_text'>neutron.conf</span> file contains a comma separated list of python classes which implement extension plugin services. The following example would load the default community LoadBalancerPlugin.
    </p>
    <p>
      In the <span class='command_text'>neutron.conf</span> file:</p>
    <p><span class='command_text'>service_plugins =
neutron.services.loadbalancer.plugin.LoadBalancerPlugin</span></p>
    <p>Once the default community LoadBalancerPlugin plugin is loaded, service provider type driver entries, in the <span class='command_text'>service_providers</span> section of the neutron server configuration, will be examined. In Icehouse and Juno the
      default location for the <span class='command_text'>service_providers</span> section was in the <span class='command_text'>/etc/neutron/neutron.conf</span> file. In Kilo the <span class='command_text'>service_providers</span> for LBaaS services
      can be found in the <span class='command_text'>/etc/neutron/neutron_lbaas.conf</span> file. Providing the following service provider type driver entry for the <span class='command_text'>LOADBALANCER</span> plugin service will load the f5 LBaaSv1
      service provider driver as the default driver for all LOADBALANCER requests:
    </p>
    <p><span class='command_text'>
service_provider=LOADBALANCER:F5:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriver:default
</span>
    </p>
    <p>You must have at least one <span class='command_text'>default</span> entry for each plugin service. If the f5 service provider driver is not your default, simply remove the <span class='command_text'>default</span> field from the above entry.
    </p>
    <p>In the default section of your <span class='command_text'>neutron.conf</span> file, the optional <span class='command_text'>f5_loadbalancer_pool_scheduler_driver</span> variable can be set to an alternative agent scheduler.
      The default value for this setting,
      <span class='command_text'>f5.oslbaasv1driver.drivers.agent_scheduler.TenantScheduler</span>, causes LBaaSv1 pools to be distributed within an environment with tenant affinity.
			You should only provide an alternative scheduler if you have an alternative service placement requirement and are capable
			of writing your own scheduler.
    </p>
    <h3>
Agent Installation
</h3>
    <p>
      The Neutron LBaaSv1 agent is distributed as a Debian or Red Hat installation package. To install the agent, download the appropriate package to the host you want to run your agent and install it. This can be the Neutron controller, but does not have to
      be.
    </p>
    <p>
      Debian / Ubuntu example:
    </p>
    <p>
      <span class='command_text'>dpkg -i f5-bigip-common_1.0.10-1_all.deb</span></br>
      <span class='command_text'>dpkg -i f5-oslbaasv1-driver_1.0.10-1_all.deb</span>
    </p>
    <p>
      Rad Hat example:
    </p>
    <p>
      <span class='command_text'>rpm -ivh f5-bigip-common-1.0.10-1.noarch.el7.rpm</span></br>
      <span class='command_text'>rpm -ivh f5-oslbaasv1-driver-1.0.10-1.noarch.el7.rpm</span>
    </p>
    <p>The installation will start a service called <span class='command_text'>f5-oslbaasv1-agent</span>. Stop this service and then make appropriate changes to the <span class='command_text'>/etc/neutron/f5-oslbaasv1-agent.ini</span> file, and then restart
      the agent. The configuration items in the <span class='command_text'>/etc/neutron/f5-oslbaasv1-agent.ini</span> file are detailed in full later in this document.
    </p>
    <p>
      The agent service will also expect to find a <span class='command_text'>/etc/neutron/neutron.conf</span> file which contains the configuration for Neutron messaging. To make sure the messaging settings match those of the controller, often it is
      easiest to simply copy the <span class='command_text'>/etc/neutron/neutron.conf</span> from the controller.
    </p>
    <h2>
<span class='mw-headline' id='Multiple Controllers and Agent Redundancy'>
Multiple Controllers and Agent Redundancy
</span>
</h2>
    <p>
      The service provider driver runs within the Neutron controller. When the service provider driver is loaded by the community LBaaS plugin, a named messaging queue is created which will be used to consume all inbound callbacks and status update requests
      from agents. By default the queue is global for all f5 LBaaSv1 agents. (To run multiple queues, see the differentiated service section below) If mulitple Neutron controllers are started, the various service provider will all listen to the same named
      message queue. This provides controller redudancy and scale out for the callback and status update calls from agents. Each service provider will consume requests off the queue in a round robin fashion. All Neutron controllers must work off the same
      Neutron database to make sure no state problems occur with concurrent controller instances running.
    </p>
    <p>
      <img src='./images/basic_agent_scheduled_redudancy.png' width='750px' />
    </p>
    <p>
      When multiple agents services are started using the same <span class='command_text'>environment_prefix</span> each agent will communicate with their configured iControl endpoints, assure that the TMOS systems meet minimal requirements, create a
      specific named queue unique to this specific agent for processing provisioning requests from service provider drivers, and then report as a valid f5 LBaaSv1 agent via the standard Neutron controller Agent status queue. The agents continue to report
      their status to the agent queue on a periodic basis, every 10 seconds by default.
    </p>
    <p>
      When a request for a new pool is made to a Neutron controller, a f5 service provider driver invokes the Tenant scheduler. The scheduler will query all f5 LBaaSv1 agents reporting active and what previously cretaed pools are bound to each. If an active
      agent is found which already has a bound pool for the same <span class='command_text'>tenant_id</span> as the new pool to be created, that agent will be selected. Otherwise an agent is selected at random for all active agents. The selected
      agent is then bound to the pool for the life time of the pool This assures that any additional tasks to be peformed on that pool will go back to the same agent. The request to create the pool service will be sent to the bound agent's specific task
      queue for processing on the agent. When complete the agent will report the outcome of the provisioning task to the LBaaSv1 callback queue where one service provider driver will process the status and update the Neutron database for the new pool.
    </p>
    <p>
      All subsequent requests for actions on a pool will be sent to the task queue for the bound agent as long as the agent has reported to Neutron as active within the agent update period. If the agent has not reported as active, assumingly because the agent
      process is no long running, the Tenant scheduler will query if there are any other agents with the same <span class='command_text'>environment_prefix</span> as the non-active the bound agent, and if one is found, the task will be sent to the
      queue for the first alive agent maching the bound agent's <span class='command_text'>environment _prefix</span>. This provides basic task redudancy, however it does not rebind the pool to the backup active agent. The expectation is that the bound
      agent will be brought back online at some point to process tasks for all of its bound pools.
    </p>
    <h2>
<span class='mw-headline' id='LBaaSv1 Differentiated Services and TMOS Scale Out'>
LBaaSv1 Differentiated Services and TMOS Scale Out
</span>
</h2>
    <p>
      The f5 LBaaSv1 service provider driver and agents support LBaaS deployments where multiple TMOS environments are required. In order to differentiate between these TMOS environments, distinct service provider entires need to be created in the Neutron controller
      configuration, one for each enivronment. This is the only way to allow a tenant to pick their environment through the LBaaS API.
    </p>
    <p>
      Each service provider driver will work as described above with the exception that each environment will create its own queue to process callbacks and status updates from agents in their environemnt. This is in contrast to the default install which creates
      one global queue to process requests. Additionally, the scheduling of tasks to agents will only consider agents within the environment.
    </p>
    <h3>Adding a Service Provider Environment to a Neutron Controller</h3>
    <p>
      The default install come with three common environment names available without generating any new drivers.
    </p>
    <table>
      <tr>
        <th>Environment</th>
        <th>Driver</th>
      </tr>
      <tr>
        <td>Dev</td>
        <td>f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverDev</td>
      </tr>
      <tr>
        <td>Test</td>
        <td>f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverTest</td>
      </tr>
      <tr>
        <td>Prod</td>
        <td>f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverProd</td>
      </tr>
    </table>
    <p>
      <p>
        You can load these environments by adding the following <span class='command_text'>service_provider</span> entries in your Neutron controller configuration and restarting the <span class='command_text'>neutron-server</span> process.
      </p>
      <p>
        <span class='command_text'>service_provider=LOADBALANCER:DEV:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverDev</span>
        <span class='command_text'>service_provider=LOADBALANCER:TEST:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverTest</span>
        <span class='command_text'>service_provider=LOADBALANCER:PROD:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverProd</span>
      </p>
      <img src='./images/driver_multiple_environments.png' />
    </p>
    <h3>Adding Agent Environments</h3>
    <p>
      The agent task scheduling works exactly as descibed above for differentiated TMOS environments with the exception that each environment will be scheduled separately from each other.
    </p>
    <p>
<b><i>Warning: </i></b> A differentiated TMOS evironment can not share anything. This precludes the use of vCMP for
differentiated environments because vCMP guests share global VLAN IDs.
   </p>
    <p>
      To place an agent into an environment, there are two setting which need to be set in the agent configuration.
    </p>
    <table>
      <tr>
        <th>f5-oslbaasv1-agent.ini setting</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>environment_prefix</td>
        <td>must match your service provider environment string,
          </br>but lowercase. In example for PROD, it would be 'prod'</td>
      </tr>
      <tr>
        <td>environment_specific_plugin</td>
        <td>true</td>
      </tr>
    </table>
    <p>
      Setting these configuration settings will cause the agents to use their service provider's specific queue for their callbacks and updates.
    </p>
    <p>
      <img src='./images/agent_multiple_environments.png' />
    </p>
    <h3>Creating Custom Environments</h3>
    <p>
      To create environments beyond the Dev, Test, and Prod environments which come with the service provider driver install, a driver generating module is use. On each Neutron controller which will host your customer environment, run the following command:
    </p>
    <p>
      <span class='command_text'>sudo python -m f5.oslbaasv1driver.utils.generate_env.py provider_name environment_prefix</span>
    </p>
    <p>
      As example, to add the environment 'DFW1', you would issue the following command:
    </p>
    <p>
      <span class='command_text'>sudo python -m f5.oslbaasv1driver.utils.generate_env.py DFW1 DFW1</span>
    </p>
    <p>
      After running this, a driver class will be generated and a <span class='command_text'>service_provider</span> entry in your Neutron controller configuration will be made:
    </p>
    <p>
      <span class='command_text'>#service_provider=LOADBALANCER:DFW1:f5.oslbaasv1driver.drivers.plugin_driver_Dfw1.F5PluginDriverDfw1</span>
    </p>
    <p>
      To activate your custom environment you will need to remove the comment '#' from the configuration and restart your
      <span class='command_text'>neutron-server</span> process.
    </p>
    <h3>Capacity Based Scale Out Per Environment</h3>
    <p>
      For a specific differentiated service environment, groups of agents can be configured, each associated with distinct iControl endpoints, and thus different TMOS devices. When grouping is specified within an environment, the service provider scheduler
      will consider the grouping along with a reported <span class='command_text'>environment_capacity_score</span>. Together, the agent grouping and the capacity score, allow the scheduler to scale out a single environment across multiple TMOS
      device service groups.
    </p>
    <p>
      To enable environment grouping, set the following in the agent configuration:
    </p>
    <p>
      <table>
        <tr>
          <th>f5-oslbaasv1-agent.ini setting</th>
          <th>Value</th>
        </tr>
        <tr>
          <td>environment_group_number</td>
          <td>integer number greater than one (1)</td>
        </tr>
      </table>
    </p>
    <p>
      All agents in the same group should have the same <span class='command_text'>environment_group_number</span> setting.
    </p>
    <p>
      Capacity is measured by each agent from their configured TMOS devices. The agent will report a single <span class='command_text'>environment_capacity_score</span> for its group every time it reports its agent status to the Neutron controller.
    </p>
    <p>
      The <span class='command_text'>environment_capacity_score</span> value will be the highest capacity recorded on several collected statistics specified in the
      <span class='command_text'>capacity_policy</span> setting in the agent configuration. The <span class='command_text'>capacity_policy</span> setting value is a dictionary where the key is the metric name and the value is the max allowed value for
      that metric. The score is determined simply by dividing the metric collected by the max for that metric specified in the <span class='command_text'>capacity_policy</span> setting. That makes an acceptable reported <span class='command_text'>environment_capacity_score</span>      between zero (0) and one (1). If an agent in the group reports a <span class='command_text'>environment_capacity_score</span> of one (1) or greater, it will be considered at capacity.
    </p>
    <p>
      <img src='./images/env_group_scale_out.png' width='750px' />
    </p>
    <p>
      The following metrics are implemented by the iControl driver:
    </p>
    <p>
      <table>
        <tr>
          <th>Metric Name</th>
          <th>Value Collected</th>
        </tr>
        <tr>
          <td>throughput</td>
          <td>total throughput in bps of the TMOS devices</td>
        </tr>
        <tr>
          <td>inbound_throughput</td>
          <td>throughput in bps inbound to TMOS devices</td>
        </tr>
        <tr>
          <td>outbound_throughput</td>
          <td>throughput in bps outbound from TMOS devices</td>
        </tr>
        <tr>
          <td>active_connections</td>
          <td>number of concurrent active actions on a TMOS device</td>
        </tr>
        <tr>
          <td>tenant_count</td>
          <td>number of tenants associated with a TMOS device</td>
        </tr>
        <tr>
          <td>node_count</td>
          <td>number of nodes provisioned on a TMOS device</td>
        </tr>
        <tr>
          <td>route_domain_count</td>
          <td>number of route domains on a TMOS device</td>
        </tr>
        <tr>
          <td>vlan_count</td>
          <td>number of VLANs on a TMOS device</td>
        </tr>
        <tr>
          <td>tunnel_count</td>
          <td>number of GRE and VxLAN overlay tunnels on a TMOS device</td>
        </tr>
        <tr>
          <td>ssltps</td>
          <td>the current measured SSL TPS count on a TMOS device</td>
        </tr>
        <tr>
          <td>clientssl_profile_count</td>
          <td>the number of clientside SSL profiles defined</td>
        </tr>
      </table>
    </p>
    <p>
      You can specify one or multiple metrics.
    </p>
    <p>
      <table>
        <tr>
          <th>f5-oslbaasv1-agent.ini setting</th>
          <th>Value</th>
        </tr>
        <tr>
          <td>capacity_policy</td>
          <td>dictionary of metrics to collect and the max value of the metric.</br>
            </br>
            For example:</br>
            </br> <span class='command_text'>capacity_policy = throughput:1000000000, active_connections: 250000, route_domain_count: 512, tunnel_count: 2048</span></td>
        </tr>
      </table>
    </p>
    <p>
      When multiple <span class='command_text'>environemnt_group_number</span> designated group of agents are available, and a pool is created where the pool's <span class='command_text'>tenant_id</span> is not associated with an environment group already,
      the scheduler will try to assign the pool to the group with the last reported lowest <span class='command_text'>environment_capacity_score</span>. If the pool's <span class='command_text'>tenant_id</span> was associated with an agent where the
      <span class='command_text'>environemnt_group_number</span> for all agents in the group are above capacity, the new pool will be bound to an agent within another group in the environment where capacity is under the limit.
    </p>
<b><i>Warning: </i></b>If you set the <span class='command_text'>capacity_policy</span> and all agent
in all groups for an environment are at capacity, services will no long be scheduled. When pools
are created for an environment which has no capacity left, the pools will be placed in the error
state.
    </p>
    <h3>Starting Multiple Agents on the Same Host</h3>
    <p>In order to start more than one agent on the same host, several alterations need to be made for each additional agent instance.
    </p>
    <b><i>Warning: </i></b>You should never run two agents <i>for the same environment</i> on the
same host, as the hostname is used to help Neutron distinquish between agents. Starting
multiple agent processes for different environments, meaning each agent associated with
different iControl endpoints, on the same host is a valid configuration.
    <p>
      To configure multiple agent processes on the same host:
    </p>
    <p>
      <ol>
        <li>Create a unique configuration file for each agent, using
          <span class='command_text'>/etc/neutron/f5-oslbaasv1-agent.ini</span> as a template. Each configuration
          file would have distinct iControl endpoints defined for its environment.
        </li>
        <li>Create additional upstart, init.d, or systemd service definitions for addition agents using the default service definitions as a guide. Each service should point to the appropriate configuration file created in the previous step.
          The agent process uses Oslo configuration. This means that typically the only thing that would change from the template service definitions would be the
          <span class='command_text'>--config-file</span> and <span class='command_text'>--log-file</span> comand line arguments used to start the <span class='command_text'>/usr/bin/f5-oslbaasv1-agent</span> executable.</li>
        <li>Start each agent using the name of its unique upstart, init.d, or systemd service name.</li>
      </ol>
    </p>
    <h2><span class='mw-headline' id=
'iControl Driver Supported TMOS Versions'>iControl Driver Supported TMOS Versions</span></h2>
    <p>The agent processes themselves include orchestration methods which are available on TMOS 11.5 and greater. The agents will check for this version of TMOS on all connected devices. If an older version of TMOS is detected, the agents will not register
      with Neutron.
    </p>
    <h2><span class='mw-headline' id=
'Supported Neutron Network Topologies'>Supported Neutron Network
Topologies</span></h2>
    <p>The F5 iControl agent driver supports the following network topologies with either hardware appliances or TMOS virtual editions:
    </p>
    <ul>
      <li>
        <p>Global routed mode where all VIPs are assumed routable from clients and all Members are assumed routable from the TMOS devices themselves. All L2 and L3 objects, including routes, must be pre-provisioned on the TMOS Device Service Group prior
          to LBaaSv1 provisioning.
        </p>
        <table>
          <tr>
            <th>Topology</th>
            <th>f5-oslbaasv1-agent.ini setting</th>
          </tr>
          <tr>
            <td><img src='./images/global_routed_mode.png' /></td>
            <td>
              <p><span class='command_text'>f5_global_routed_mode =
True</span></p>
            </td>
          </tr>
        </table>
        <p>Global routed mode uses TMOS AutoMap SNAT for all VIPs. Because no explicit SNAT pools are being defined, sufficient Self IP addresses should be created to handle connection loads.</p>
        <p><b><i>Warning:</i></b>In global routed mode, because all access to and from the TMOS devices is assumed globally routed, there is no network segregation between tenant services on the TMOS devices themselves. Overlapping IP address spaces for tenant
          objects is likewise not available.</p>
      </li>
      <li>
        <p>One-Arm mode where VIP and Members can be provisioned from the same Neutron subnet.</p>
        <table>
          <tr>
            <th>Topology</th>
            <th>f5-oslbaasv1-agent.ini setting</th>
          </tr>
          <tr>
            <td><img src='./images/one_arm.png' /></td>
            <td>
              <p><span class='command_text'>f5_global_routed_mode = False<br />
f5_snat_mode = True<br /></span></p>
              <p>
                <br /> optional settings:
                <br />
              </p>
              <p><span class='command_text'>f5_snat_addresses_per_subnet =
n<br /></span></p>
              <p>where if n is 0, the virtual server will use AutoMap SNAT. If n is &gt; 0, n number of SNAT addresses will be allocated from the Member subnet per active traffic group.</p>
            </td>
          </tr>
        </table>
      </li>
      <li>
        <p>Multiple-Arm mode where VIP and Members are provisioned from different Neutron subnets.</p>
        <table>
          <tr>
            <th>Topology</th>
            <th>f5-oslbaasv1-agent.ini setting</th>
          </tr>
          <tr>
            <td><img src='./images/multiarm_snat.png' /></td>
            <td>
              <p><span class='command_text'>f5_global_routed_mode = False<br />
f5_snat_mode = True<br /></span></p>
              <p>
                <br /> optional settings:
                <br />
              </p>
              <p><span class='command_text'>f5_snat_addresses_per_subnet =
n<br /></span></p>
              <p>where if n is 0, the virtual server will use AutoMap SNAT. If n is &gt; 0, n number of SNAT addresses will be allocated from the Member subnet per active traffic group.</p>
            </td>
          </tr>
        </table>
      </li>
      <li>
        <p>Gateway routed mode where attemps will be made to create a default gateway forwarding service on the TMOS Device Service Group for Member Neutron subnets</p>
        <table>
          <tr>
            <th>Topology</th>
            <th>f5-oslbaasv1-agent.ini setting</th>
          </tr>
          <tr>
            <td><img src='./images/routed_mode.png' /></td>
            <td>
              <p><span class='command_text'>f5_global_routed_mode = False<br />
f5_snat_mode = False<br /></span></p>
            </td>
          </tr>
        </table>
      </li>
    </ul>
    <p>For the Neutron network topologies requiring dynamic L2 and L3 provisioning of the TMOS devices, which includes all network topologies except global routed mode, the f5 LBaaSv1 iControl driver supports:
    </p>
    <ul>
      <li>Provider VLANs - VLANs defined by the admin tenant and shared
      </li>
      <li>Tenant VLANs - VLANs defined by the admin tenant for other tenants or by the tenant themselves</li>
      <li>Tenant GRE Tunnels - GRE networks defined by the tenant</li>
      <li>Tenant VxLAN Tunnels - VxLAN networks defined by the tenant
      </li>
    </ul>
    <h3>VLANs</h3>
    <p>For VLAN connectivity, the f5 TMOS devices use a mapping between the Neutron network <span class='command_text'>provider:physical_network</span> attribute and TMM interface names. This is analogous to the Open vSwitch agents mapping between the Neutron
      network <span class='command_text'>provider:physical_network</span> and their interface bridge name. The mapping is created in the <span class='command_text'>f5-oslbaasv1-agent.ini</span> configuration file using in the <span class='command_text'>f5_external_physical_mappings</span>      setting. The name of the <span class='command_text'>provider:physical_network</span> entries can be added to a comma separated list with mappings to the TMM interface or LAG trunk name, and a boolean attribute to specify if 802.1q tagging will be
      applied. An example which would map the
      <span class='command_text'>provider:physical_network</span> containing 'ph-eth3' to TMM interface 1.1 with 802.1q tagging would look like this:</p>
    <div style="margin-left: 2em"><span class='command_text'>f5_external_physical_mappings =
ph-eth3:1.1:True</span></div>
    <p>A default mapping should be included for cases where the
      <span class='command_text'>provider:physical_network</span> does not match any configuration settings. A default mapping simply uses the word <span class='command_text'>default</span> instead of a known <span class='command_text'>provider:physical_network</span>      attribute.
    </p>
    <p>An example that would include the previously illustrated mapping, a default mapping, and LAG trunk mapping, might look like this:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>f5_external_physical_mappings = default:1.1:True,
ph-eth3:1.1:True, ph-eth4:lag-trunk-1:True</span></div>
     <p>
     <b><i>Warning: </i></b> The default Open vSwitch Neutron networking does not support
    VLAN tagging by guest instances. Each guest interface is treated as an access port and all VLAN tags will be stripped
     before frames reach the physical network infrastructure. To allow a TMOS VE guest to function in L2 Adajent mode
     using VLANs as your tenant network type, the software networking infrastructure which strips VLAN tags from
     frames must be bypassed. Bypassing the software bridge can be accomplished used the <span class='command_text'>ip</span>,
     <span class='command_text'>brctl</span>, and <span class='command_text'>ovs-vsctl</span> commands on the compute node
     after the TMOS VE guest instances has been created. It is not a process which is automated by any Neutorn agent. This
     requirement only applies to TMOS VE when running as a Nova guest instance.
     </p>
     <p>
     <img src='./images/VE_Multitenant_VLAN_bypass.png' />
     </p>
    <h3>Tunnels</h3>
    <p>For GRE and VxLAN tunnels, the f5 TMOS devices expect to communicate with Open vSwitch VTEPs. The VTEP addresses for Open vSwitch VTEPs are learned from their registered Neutron agent configurations <span class='command_text'>tunneling_ip</span> attribute.
      I.E.:</p>
    <pre>
# neutron agent-show 034bddd0-0ac3-457a-9e2c-ed456dc2ad53
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| agent_type          | Open vSwitch agent                   |
| alive               | True                                 |
| binary              | neutron-openvswitch-agent            |
| configurations      | {                                    |
|                     |      "tunnel_types": [               |
|                     |           "gre"                      |
|                     |      ],                              |
|                     |      "<b>tunneling_ip</b>": "10.1.0.35",    |
|                     |      "bridge_mappings": {            |
|                     |           "ph-eth3": "br-eth3"       |
|                     |      },                              |
|                     |      "l2_population": true,          |
|                     |      "devices": 4                    |
|                     | }                                    |
| created_at          | 2013-11-15 05:00:23                  |
| description         |                                      |
| heartbeat_timestamp | 2014-04-22 16:58:21                  |
| host                | sea-osp-cmp-001                      |
| id                  | 034bddd0-0ac3-457a-9e2c-ed456dc2ad53 |
| started_at          | 2014-04-17 22:39:30                  |
| topic               | N/A                                  |
+---------------------+--------------------------------------+

</pre>
    <p>The ML2 L2 Population service is supported by the f5 LBaaSv1 agent iControl driver, such that only Open vSwitch agents hosting Members will have overlay tunnels built to them for Member IP access. When the ML2 L2 Population service is used, static ARP entries
      will optionally be created on the TMOS devices to remove the need for them to send ARP broadcast (flooding) across the tunnels to learn the location of Members. In order to support this, the ML2 port binding extensions and segmentation models must
      be present. The port binding extensions and segmentation model are defined by default with the community ML2 core plugin and Open vSwitch agents on the compute nodes.</p>
    <p>When VIPs are placed on tenant overlay networks, the f5 LBaaSv1 agent will send tunnel update RPC messages to the Open vSwitch agents informing them of TMOS device VTEPs. This enables tenant guest virtual machines or network node services to interact
      with the TMOS provisioned VIPs across overlay networks. An f5 LBaaSv1 agent's connected TMOS VTEP addresses are placed in the agent's configurations and reported to Neutron. The VTEP addresses are listed as <span class='command_text'>tunneling_ips</span>.</p>
    <pre>

      # neutron agent-show 014ada1a-91ab-4408-8a81-7be6c4ea8113
      +---------------------+-----------------------------------------------------------------------+
      | Field               | Value                                                                 |
      +---------------------+-----------------------------------------------------------------------+
      | admin_state_up      | True                                                                  |
      | agent_type          | Loadbalancer agent                                                    |
      | alive               | True                                                                  |
      | binary              | f5-bigip-lbaas-agent                                                  |
      | configurations      | {                                                                     |
      |                     |      "icontrol_endpoints": {                                          |
      |                     |           "10.0.64.165": {                                            |
      |                     |                "device_name": "host-10-0-64-165.openstack.f5se.com",  |
      |                     |                "platform": "Virtual Edition",                         |
      |                     |                "version": "BIG-IP_v11.6.0",                           |
      |                     |                "serial_number": "b720f143-a632-464c-4db92773f2a0"     |
      |                     |           },                                                          |
      |                     |           "10.0.64.164": {                                            |
      |                     |                "device_name": "host-10-0-64-164.openstack.f5se.com",  |
      |                     |                "platform": "Virtual Edition",                         |
      |                     |                "version": "BIG-IP_v11.6.0",                           |
      |                     |                "serial_number": "e1b1f439-72c3-5240-4358bbc45dff"     |
      |                     |           }                                                           |
      |                     |      },                                                               |
      |                     |      "request_queue_depth": 0,                                        |
      |                     |      "environment_prefix": "dev",                                     |
      |                     |      <b>"tunneling_ips":  </b>                                               |
      |                     |      <b>     "10.0.63.126", </b>                                             |
      |                     |      <b>     "10.0.63.125"  </b>                                             |
      |                     |      <b>]</b>,                                                               |
      |                     |      "common_networks": {},                                           |
      |                     |      "services": 0,                                                   |
      |                     |      "environment_capacity_score": 0,                                 |
      |                     |      "tunnel_types": [                                                |
      |                     |           "gre"                                                       |
      |                     |      ],                                                               |
      |                     |      "environment_group_number": 1,                                   |
      |                     |      "bridge_mappings": {                                             |
      |                     |           "default": "1.3"                                            |
      |                     |      },                                                               |
      |                     |      "global_routed_mode": false                                      |
      |                     | }                                                                     |
      | created_at          | 2015-08-19 13:08:15                                                   |
      | description         |                                                                       |
      | heartbeat_timestamp | 2015-08-20 15:19:15                                                   |
      | host                | sea-osp-ctl-001:f5acc0d3-24d6-5c64-bc75-866dd26310a4                  |
      | id                  | 014ada1a-91ab-4408-8a81-7be6c4ea8113                                  |
      | started_at          | 2015-08-19 17:30:44                                                   |
      | topic               | f5-lbaas-process-on-agent                                             |
      +---------------------+-----------------------------------------------------------------------+
</pre>
    <h2><span class='mw-headline' id='OpenStack and TMOS Multinenancy'>OpenStack and TMOS Multinenancy</span></h2>
    <p>By default, all TMOS objects are created in administrative partitions associated with the OpenStack <span class='command_text'>tenant_id</span> for the Pool.</p>
    <p>If the <span class='command_text'>f5-oslbaasv1-agent.ini</span> configuration file setting for <span class='command_text'>use_namespaces</span> is set to <span class='command_text'>True</span>, and it is not configured for global routed mode, each
      tenant will also have a TMOS route domain created, providing segmentation for IP address spaces between tenants.</p>
    <p>If an associated Neutron network for a VIP or Member is marked as <span class='command_text'>shared=True</span>, and the f5 LBaaSv1 agent is not in global routed mode, all associated L2 and L3 objects will be created in the <span class='command_text'>/Common</span>      administrative partition and associated with route domain 0 (zero) on all TMOS devices.</p>
    <p><img src='./images/tenancy_mapping.png' /></p>

    <h2><span class='mw-headline' id='Supported TMOS High Availability Modes'>Supported TMOS High Availability Modes</span></h2>
    <p>The F5 iControl agent driver supports:</p>
		<ul>
      <li>Standalone - No High Availability</li>
      <li>Pair mode - Active / Standby TMOS devices</li>
      <li>ScaleN mode - Multiple Active TMOS devices, up to 4 (four) devices</li>
    </ul>
    <h2><span class='mw-headline' id='Agent Configuration Items'>Agent Configuration Items</span>
</h2>
    <p>The f5 LBaaSv1 agent has many configurable options which will effect its provisioning behavior.
    </p>
    <p>
      The installation process will automatically start an agent process as a service. You will need to stop the agent service, configure the
      <span class='command_text'>/etc/neutron/f5-oslbaasv1-agent.init</span> file appropriately, and then restart the agent process.
    </p>
    <p>The default configuration file for the f5 LBaaSv1 agent is located at <span class='command_text'>/etc/neutron/f5-oslbaasv1-agent.ini</span>. Below is a consideration of each setting in that configuration file. The configuration file installed with
      the agent package contains extensive documention comments as well.</p>
    <table border='1' width='1024px' cellpadding='2'>
      <tbody>
        <tr>
          <th nowrap="nowrap">Setting</th>
          <th nowrap="nowrap">Allowed Values</th>
          <th nowrap="nowrap">Default Value</th>
          <th nowrap="nowrap">Description</th>
        </tr>
        <tr>
        <th colspan=4>General Settings</th>
        </tr>
        <tr>
          <td nowrap="nowrap">debug</td>
          <td>True or False</td>
          <td>False</td>
          <td>Should the agent create verbose debug logging in /var/log/neutron/f5-oslbaasv1-agent.log. You should remember to check the /etc/neutron/neutron.conf settings for debug and verbose as well.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">periodic_interval</td>
          <td>integer number of seconds</td>
          <td>10</td>
          <td>How often the statistics are updated for LBaaSv1 objects assoicated with this agent.
          </td>
        </tr>
        <tr>
          <td nowrap="nowrap">service_resync_interval</td>
          <td>integer number of seconds</td>
          <td>500</td>
          <td>How often the agent pulls known configurations from Neutron and syncrhonizes and cleans up orphaned configurations on the BIG-IPs.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">use_namespace</td>
          <td>True or False</td>
          <td>True</td>
          <td>Should all Neutron non-shared subnet addresses be assigned to route domains</td>
        </tr>
        <tr>
          <td nowrap="nowrap">max_namespaces</td>
          <td>integer number of namespaces</td>
          <td>1</td>
          <td>If a tenant attempts to create overlapping subnets within their own networks, this number must be increased to the max number of overlapping subnets that will be supported. By default only one route domain will be created per tenant. If this
            is increased, it can adversely effect the tenant scaling of the BIG-IPs as maintaining separate routing tables cost control plan memory.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">static_agent_configuration_data</td>
          <td>single or comma separated list</td>
          <td>None</td>
          <td>This list of name:value attribues will be sent as part of the agent configuration data to the plugin.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_device_type</td>
          <td>external</td>
          <td>external</td>
          <td>This determines the type of automatica device onboarding. Only external is acceptable with this releaes.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_ha_type</td>
          <td>standalone, pair, or scalen</td>
          <td>pair</td>
          <td>This determines the way LBaaSv1 will be deployed for HA.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_sync_mode</td>
          <td>auto_sync or replication</td>
          <td>replication</td>
          <td>This determines the way LBaaSv1 will be provisioned. For auto_sync, only a single TMOS device will be provisioned and DSG syncing used to replicate the configuration objects. For replication, each device will be provisioned independantly. Only
            replication mode is supported at this time.</td>
        </tr>
        <tr>
        <th colspan=4>Environment Settings</th>
        </tr>
        <tr>
          <td nowrap="nowrap">environment_prefix</td>
          <td>String prefix</td>
          <td>'' (blank string)</td>
          <td>This is the prefix this agent will use for all BIG-IP objects. Default is a blank string which will result in BIG-IP objects prefixed with uuid_.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">environment_specific_plugin</td>
          <td>True or False</td>
          <td>False</td>
          <td>Setting this to True will cause the agent to try and communicate with a service differentiated plugin driver associated with its environment.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">environment_group_number</td>
          <td>integer group number</td>
          <td>1</td>
          <td>This puts the agent in a specific group used when scaling out an environment beyound one service group of BIG-IPs.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">capacity_policy</td>
          <td>dictionary of metrics and max values</td>
          <td>None</td>
          <td>A policy of metric names and max values for this device service group used to report environment capacity. This is used to determine group scale out.</td>
        </tr>
        <tr>
        <th colspan=4>VLAN Settings</th>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_external_physical_mappings</td>
          <td>comma separated list</td>
          <td>default:1.1:True</td>
          <td>Determines how Neutron VLAN networks are created. The format for each mapp entry is [provider:physical_network]:[TMOS interface or trunk name]:[Boolean to tag the VLAN ]. One entry with 'default' as it initial entry should be present to allow
            for cases where the provider:physical_network does not match any other specified mapping.
          </td>
        </tr>
        <tr>
          <td>vlan_binding_driver</td>
          <td>vlan binding driver path</td>
          <td>f5.oslbaasv1agent.drivers.bigip. vlan_binding.NullBinding
          </td>
          <td>
            Some systems require the need to bind and prune VLANs ids allowed to specific ports, often for security.
          </td>
        </tr>
        <tr>
          <td>interface_port_static_mappings</td>
          <td>JSON dictionary</td>
          <td>'' (blank string)</td>
          <td>
            The interface_port_static_mappings allows for a JSON encoded dictionary mapping BigIP devices and interfaces to corresponding ports.
          </td>
        </tr>
        <tr>
        <th colspan=4>Tunnel Settings</th>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_vtep_folder</td>
          <td>TMOS folder name</td>
          <td>Common</td>
          <td>For tenant tunneling, this determines the TMOS folder to discover the VTEP interface.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_vtep_selfip_name</td>
          <td>TMOS Self IP name</td>
          <td>vtep</td>
          <td>For tennant tunneling, this determines the pre-provisioned VTEP Self IP name.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">advertised_tunnel_types</td>
          <td>gre
            <br /> vxlan
            <br /> gre,vxlan
          </td>
          <td>gre,vxlan</td>
          <td>Specifies the tunnel types the agent advertises via RPC to other VTEPs. This should match your ML2 allowed network_typs. Default is to advertise both gre and vxlan.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_populate_static_arp</td>
          <td>True or False</td>
          <td>False</td>
          <td>Specifies that if a Pool Member IP address is associated with a GRE or VxLAN tunnel network, in addition to a tunnel fdb record being added, that a static arp entry will be created to avoid the need to learn the Member's MAC address via flooding.</td>
        </tr>
        <td nowrap="nowrap">l2_population</td>
        <td>True or False</td>
        <td>True</td>
        <td>The agent will register for Neutron L2 population updates and will populate tunnel fdb entries accordingly. Without L2 population, pool member migration will not function properly.</td>
        </tr>
        <tr>
        <th colspan=4>L3 Settings</th>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_global_routed_mode</td>
          <td>True or False</td>
          <td>False</td>
          <td>If set, only L4+ objects will be provisioned. All necessary L2 and L3 routes will be assumed pre-provisioned. SNAT will be truned to AutoMAP on all VIPs, and all L2 and L3 tenancy isolation will not be provisioned.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_snat_mode</td>
          <td>True or False</td>
          <td>True</td>
          <td>For L3 forwarding, should SNAT be used. If set to False, the LBaaSv1 device will attempt to become the default gateway for the Neutron subnets associated with Member objects.</td>
        </tr>
        <tr>
          <td>f5_snat_addresses_per_subnet</td>
          <td>integer number of SNAT addresses</td>
          <td>1</td>
          <td>If set to 0(zero), AutoMAP SNAT will be used. If set to a positive integer, this number of SNAT addresses will be created per Neutron subnet associated with Pool Members, per active TMOS device in an HA mode.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_route_domain_strictness</td>
          <td>True or False</td>
          <td>False</td>
          <td>If set to True, tenant route domain isolation will be set to strict and VIPs and Members must be placed in the same route domain. This would mean that all Neutron networks with the shared attribute can not be used for VIPs or Members.</td>
        </tr>
        <tr>
          <td nowrap="nowrap">f5_common_external_networks</td>
          <td>True or False</td>
          <td>False</td>
          <td>If set to True, any VIP address or Member address associated with a Neutron network with router:external set to True will be created in the Common parition and in the default route domain.
          </td>
        </tr>
        <tr>
          <td nowrap="nowrap">common_network_ids</td>
          <td>comma separated list</td>
          <td>None</td>
          <td>List in [vlan]:[neutron network UUID] format. All defined network UUIDs defined will exempt services from creating new VLANs, but will use the /Common/[vlan] specified instead.</td>
        </tr>
        <tr>
          <td>l3_binding_driver</td>
          <td>l3 binding driver path</td>
          <td>f5.oslbaasv1agent.drivers.bigip. l3_binding.AllowedAddressPairs
          </td>
          <td>Some system require notification when L3 addresses are bound to ports to open security features or provide forwarding information.</td>
        </tr>
        <tr>
          <td>l3_binding_static_mappings</td>
          <td>JSON dictionary</td>
          <td>'' (blank string)</td>
          <td>Much like common network ids, if there are known subnets which are bound to BIG-IP neutron ports which need L3 binding, these can be specified statically.</td>
        </tr>
        <tr>
        <th colspan=4>Device Driver Settings</th>
        </tr>
        <tr>
          <td>f5_bigip_LBaaSv1_device_driver</td>
          <td>python class name of the device driver to use</td>
          <td>
            neutron.services.loadbalancer.drivers. f5.bigip.icontrol_driver.iControlDriver</td>
          <td>Only the
            <br/> neutron.services.loadbalancer.drivers. f5.bigip.icontrol_driver.iControlDriver
            <br/>is supplied.</td>
        </tr>
        <tr>
          <td>icontrol_hostname</td>
          <td>either a single entry or a comma separated list iControl endpoint IP address or FQDNs</td>
          <td>192.168.1.245</td>
          <td>The iControl endpoint to connect to for this agent. If a comma separated list is given, you must include each device in your DSG. If only one address is given, the other device's management address will be learned from the DSG and must be reachable
            from the LBaaSv1 agent.
          </td>
        </tr>
        <tr>
          <td>icontrol_username</td>
          <td>valid username for the TMOS device</td>
          <td>admin</td>
          <td>The iControl endpoint username for this agent</td>
        </tr>
        <tr>
          <td>icontrol_password</td>
          <td>valid password for the TMOS device</td>
          <td>admin</td>
          <td>The iControl endpoint password for this agent</td>
        </tr>
        <tr>
          <td>icontrol_connection_retry_interval</td>
          <td>integer number of seconds</td>
          <td>10</td>
          <td>How often to attempt to reconnect if iControl connection fails
          </td>
        </tr>
        <tr>
          <td>icontrol_vcmp_hostname</td>
          <td>iControl endpoint IP address or FQDN</td>
          <td>192.168.1.245</td>
          <td>VCMP host which must have use the same credentials as the BIG-IP guests.</td>
        </tr>
        <tr>
          <td>icontrol_config_mode</td>
          <td>objects or iapp</td>
          <td>objects</td>
          <td>Determines if iControl objects or the LBaaSv1 iApp are used to provision services. iApp support is experimental in this release.</td>
        </tr>
        <tr>
          <td>bigiq_hostname</td>
          <td>BIG-IQ API endpoint IP or FQDN</td>
          <td>bigiqhostname</td>
          <td>Single tenant LBaaSv1 support via BIG-IQ. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>bigiq_admin_password</td>
          <td>valid password for a BIG-IQ device</td>
          <td>admin</td>
          <td>Single tenant LBaaSv1 support via BIG-IQ. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>bigip_management_username</td>
          <td>valid username for a BIG-IP device</td>
          <td>admin</td>
          <td>The username used by BIG-IQ to manage BIG-IPs for Single tenant LBaaSv1 support via BIG-IQ. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>bigip_management_password</td>
          <td>valid password for a BIG-IP device</td>
          <td>admin</td>
          <td>The password used by BIG-IQ to manage BIG-IPs for Single tenant LBaaSv1 support via BIG-IQ. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>openstack_keystone_url</td>
          <td>URL</td>
          <td>http://keystonserver:5000/v2.0</td>
          <td>OpenStack keystone instance. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>openstack_admin_username</td>
          <td>Keystone admin username</td>
          <td>valid Keystone username</td>
          <td>Admin OpenStack login to determine if a tenant has BIG-IPs which can be used for Single Tenant LBaaSv1. This is experimental in this release.</td>
        </tr>

        <tr>
          <td>openstack_admin_password</td>
          <td>Keystone admin password</td>
          <td>valid Keystone password</td>
          <td>Admin OpenStack login to determine if a tenant has BIG-IPs which can be used for Single Tenant LBaaSv1. This is experimental in this release.</td>
        </tr>
      </tbody>
    </table>
    <h2><span class='mw-headline' id='Preparing Your TMOS Device for LBaaSv1'>Preparing Your TMOS Device
for LBaaSv1</span></h2>
    <p>TMOS devices must be on-boarded to support their appropriate high availability mode. All of the following on-boarding steps can be automated using iControl. Below is a listing of all the manual CLI commands needed to complete the TMOS device on-boarding.</p>
    <h3>Standalone High Availability Mode</h3>
    <p>The TMOS device must be licensed to support Local Traffic (LTM). The TMOS device should have Local Traffic (LTM) provisioned as Nominal.
    </p>
    <p>To license a TMOS device via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>SOAPClientClient --interactive
&lt;license_basekey&gt;</span></div>
    <p>To provision the Local Traffic (LTM) to nominal via the CLI, run:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys provision ltm level nominal</span>
      <br />
      <span class='command_text'>tmsh modify sys db provision.extramb
value 500</span></div>
    <p>For Standalone Mode, SNAT addresses will get created on, and VIPs will get scheduled to, the default traffic-group-local-only non-floating traffic group.</p>
    <h3>Pair High Availability Mode</h3>
    <p>Both TMOS devices must be licensed to support Local Traffic (LTM). The TMOS devices should have Local Traffic (LTM) provisioned as Nominal.</p>
    <p>To license a TMOS device via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>SOAPClientClient --interactive
&lt;license_basekey&gt;</span></div>
    <p>To provision the Local Traffic (LTM) to nominal via the CLI, run:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys provision ltm level nominal</span>
      <br />
      <span class='command_text'>tmsh modify sys db provision.extramb
value 500</span></div>
    <p>Both TMOS devices must have their Network Time Protocol servers set to the same NTP server. Both TMOS devices should also have their timezone set to the same timezone.</p>
    <p>To set a TMOS device NTP server via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys ntp servers replace-all-with { &lt;ip_address_ntp_1&gt;
&lt;ip_address_ntp_2&gt; }</span></div>
    <p>To set a TMOS device timezone via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys ntp timezone UTC</span></div>
    <p>Both TMOS devices must share a common VLAN for HA management communication. Both devices will require a local IP address to communicate cluster heartbeat and synchronization information.</p>
    <p>To provision a TMOS device HA VLAN via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create net vlan HA interfaces add { &lt;TMOS_interface_name&gt; }
description HA</span></div>
    <p>To provision a TMOS device IP address on the HA VLAN via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create net self HA &lt;ip_address/cidr&gt; allow-service default
vlan HA</span></div>
    <p>Both TMOS devices must have their configuration sync interfaces, connection mirroring, and failover addresses defined.</p>
    <p>To define a TMOS device configuration sync address via the CLI, run:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; configsync-ip
&lt;ha_ip_address&gt;</span></div>
    <p>To define a TMOS device connection mirroring address via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; mirror-ip
&lt;ha_ip_address&gt;</span></div>
    <p>To define a TMOS device failover address via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; unicast-address { { ip
&lt;ha_ip_address&gt; port 1026 } { ip
&lt;device_mgmt_ip_address&gt; port 1026 } }</span></div>
    <p>Both TMOS devices must have distinct device names before they can be placed in a Device Service Group. Once they have distinct device names, a trust must be built between the devices.</p>
    <p>To reset TMOS device name via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh mv cm
device &lt;original_name&gt; &lt;new_name&gt;</span></div>
    <p>To add a trusted peer <i>from your primary TMOS device</i> via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm trust-domain /Common/Root ca-devices add {
&lt;peer_mgmt_ip_address&gt; } name &lt;remove_device_name&gt;
username &lt;remote_device_admin_username&gt; password
&lt;remote_device_admin_password&gt;</span></div>
    <p>Both TMOS devices must be placed into a sync failover Device Service Group to allow active/standby failover. An initial sync must be run on the new sync failover Device Service Group.</p>
    <p>To create the sync failover Device Service Group <i>from your
primary TMOS device</i> via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create cm device-group&#194;&nbsp;&lt;device_group_name&gt; devices
add
{&#194;&nbsp;&lt;primary_device_name&gt;&#194;&nbsp;&lt;remote_device_name&gt;
} type sync-failover auto-sync enabled network-failover
enabled</span></div>
    To force an initial sync for the sync failover Device Service Group
    <i>from your primary TMOS device</i> via the CLI, run:
    <div style="margin-left: 2em"><span class='command_text'>tmsh run
cm config-sync to-group &lt;device_group_name&gt;</span></div>
    <p>For Pair High Availability Mode, SNAT addresses will get created on, and VIPs will get scheduled to, the default traffic-group-1 floating traffic group.</p>
    <h3>ScaleN High Availability Mode</h3>
    <p>All TMOS devices in your ScaleN group must be licensed to support Local Traffic (LTM). The TMOS device should have Local Traffic (LTM) provisioned as Nominal.</p>
    <p>To license a TMOS device via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>SOAPClientClient --interactive
&lt;license_basekey&gt;</span></div>
    <p>To provision the Local Traffic (LTM) to nominal via the CLI, run:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys provision ltm level nominal</span>
      <br />
      <span class='command_text'>tmsh modify sys db provision.extramb
value 500</span></div>
    <p>All TMOS devices must have their Network Time Protocol servers set to the same NTP server. Both TMOS devices should also have their timezone set to the same timezone.</p>
    <p>To set a TMOS device NTP server via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys ntp servers replace-all-with { &lt;ip_address_ntp_1&gt;
&lt;ip_address_ntp_2&gt; }</span></div>
    <p>To set a TMOS device timezone via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify sys ntp timezone UTC</span></div>
    <p>All TMOS devices must share a common VLAN for HA management communication. All devices will require a local IP address to communicate cluster heartbeat and synchronization information.</p>
    <p>To provision a TMOS device HA VLAN via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create net vlan HA interfaces add { &lt;TMOS_interface_name&gt; }
description HA</span></div>
    <p>To provision a TMOS device IP address on the HA VLAN via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create net self HA &lt;ip_address/cidr&gt; allow-service default
vlan HA</span></div>
    <p>All TMOS devices must have their configuration sync interfaces, connection mirroring, and failover addresses defined.</p>
    <p>To define a TMOS device configuration sync address via the CLI, run:
    </p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; configsync-ip
&lt;ha_ip_address&gt;</span></div>
    <p>To define a TMOS device connection mirroring address via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; mirror-ip
&lt;ha_ip_address&gt;</span></div>
    <p>To define a TMOS device failover address via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm device &lt;device_name&gt; unicast-address { { ip
&lt;ha_ip_address&gt; port 1026 } { ip
&lt;device_mgmt_ip_address&gt; port 1026 } }</span></div>
    <p>All TMOS devices must have distinct device names before they can be placed in a Device Service Group. Once they have distinct device names, a trust must be built between the devices.</p>
    <p>To reset TMOS device name via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh mv cm
device &lt;original_name&gt; &lt;new_name&gt;</span></div>
    <p>To add a trusted peer <i>from your primary TMOS device</i> for each peer device via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
modify cm trust-domain /Common/Root ca-devices add {
&lt;peer_mgmt_ip_address&gt; } name &lt;remove_device_name&gt;
username &lt;remote_device_admin_username&gt; password
&lt;remote_device_admin_password&gt;</span></div>
    <p>All TMOS devices must be placed into a sync failover Device Service Group to allow active/standby failover. An initial sync must be run on the new sync failover Device Service Group.</p>
    <p>To create the sync failover Device Service Group <i>from your
primary TMOS device</i> via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create cm device-group&#194;&nbsp;&lt;device_group_name&gt; devices
add
{&#194;&nbsp;&lt;primary_device_name&gt;&#194;&nbsp;&lt;remote_device_name&gt;
.. &lt;additional_remote_device_name&gt; } type sync-failover
auto-sync enabled network-failover enabled</span></div>
    To force an initial sync for the sync failover Device Service Group
    <i>from your primary TMOS device</i> via the CLI, run:
    <div style="margin-left: 2em"><span class='command_text'>tmsh run
cm config-sync to-group &lt;device_group_name&gt;</span></div>
    <p>To enable failover between the active devices you must create traffic groups. The scheme you choose to create traffic groups will depend on your failover group design. The iControl driver in ScaleN failover mode will schedule VIPs and SNAT addresses
      on traffic groups which are not named traffic-group-1 or traffic-group-local-only. It is required that you create unique named traffic groups for you failover pattern which are associated with an appropriate HA failover order. The iControl driver
      will schedule the VIP traffic group placement based on which unique named traffic group has least number of VIPs associated with it.
    </p>
    <p>To create one of multiple traffic groups with explicit HA order
      <i>from your primary TMOS device</i> via the CLI, run:</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create cm traffic-group &lt;traffic_group_name&gt; default-device
&lt;device_name&gt; auto-failback-enabled true ha-order {
&lt;secondary_device_name&gt; .. &lt;additional_device_name&gt;
}</span></div>
    <h3>Tenant Tunnel VTEP Self IPs</h3>
    <p>In order to terminate GRE or VxLAN tenant tunnels each TMOS device must have VTEP non-floating Self IP addresses configured. The folder and name of the VTEP interface must correspond to the
      <span class='command_text'>f5_vtep_folder</span> and <span class='command_text'>f5_vtep_selfip_name</span> settings for the agent. The VTEP Self IPs must be able to route IP packets to the rest of the Open vSwitch VTEP addresses.</p>
    <p>To create a VTEP Self IP via the CLI,</p>
    <div style="margin-left: 2em"><span class='command_text'>tmsh
create net self [vtep selfip name] &lt;ip_address/cidr&gt;
allow-service default vlan [vtep VLAN name]</span></div>
    <p>A connectivity test should be made from each TMOS device to Open vSwitch VTEP address to ensure L3 routing is functioning properly.
    </p>
    <h2><span class='mw-headline' id=
'Troubleshooting_Issues'>Troubleshooting Issues</span></h2>
    <p>To troubleshoot problems with the F5 LBaaSv1 Type driver or an agent process, set the global Neutron setting and agent process
      <span class='command_text'>debug</span> setting to <span class='command_text'>True</span>. Extensive logging will then appear in the <span class='command_text'>neutron-server</span> and
      <span class='command_text'>f5-bigip-LBaaSv1-agent</span> log files on their respective hosts.</p>
  </div>
  <div id='footer' name='footer'>&#169; 2014,2015 F5 Networks</div>
</body>

</html>
