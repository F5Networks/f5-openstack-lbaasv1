<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
"http://www.w3.org/TR/html4/strict.dtd">
<!-- 
# Copyright 2014 F5 Networks Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
-->
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
		<title>Neutron/LBaaS/F5BigIP</title>
		<meta name="author" content="j.gruber@f5.com" />
		<style>
			body {
			  margin-left: 10px;
			  margin-right: 10px;
			  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
			  font-size: 14px;
			  line-height: 20px;
			  color: #333333;
			  background-color: #ffffff;
			}
			a {
			  color: #0088cc;
			  text-decoration: none;
			}
			a:hover,
			a:focus {
			  color: #005580;
			  text-decoration: underline;
			}
			h1,
			h2,
			h3,
			h4,
			h5,
			h6 {
			  margin: 10px 0;
			  font-family: inherit;
			  font-weight: bold;
			  line-height: 20px;
			  color: inherit;
			  text-rendering: optimizelegibility;
			}
			h1 small,
			h2 small,
			h3 small,
			h4 small,
			h5 small,
			h6 small {
			  font-weight: normal;
			  line-height: 1;
			  color: #999999;
			}
			h1,
			h2,
			h3 {
			  line-height: 40px;
			}
			h1 {
			  font-size: 38.5px;
			}
			h2 {
			  font-size: 31.5px;
			}
			h3 {
			  font-size: 24.5px;
			}
			h4 {
			  font-size: 17.5px;
			}
			h5 {
			  font-size: 14px;
			}
			h6 {
			  font-size: 11.9px;
			}
			h1 small {
			  font-size: 24.5px;
			}
			h2 small {
			  font-size: 17.5px;
			}
			h3 small {
			  font-size: 14px;
			}
			h4 small {
			  font-size: 14px;
			}
			ul,
		ol {
		  padding: 0;
		  margin: 0 0 10px 25px;
		}
		ul ul,
		ul ol,
		ol ol,
		ol ul {
		  margin-bottom: 0;
		}
		li {
		  line-height: 20px;
		}
		hr {
		  margin: 20px 0;
		  border: 0;
		  border-top: 1px solid #eeeeee;
		  border-bottom: 1px solid #ffffff;
		}
		table {
		  max-width: 100%;
		  background-color: transparent;
		  border-collapse: collapse;
		  border-spacing: 0;
		}
		table th,
		table td {
		  padding: 8px;
		  line-height: 20px;
		  text-align: left;
		  vertical-align: top;
		  border-top: 1px solid #dddddd;
		}
		table th {
		  font-weight: bold;
		  white-space: nowrap;
		}
		table tbody {
		  border-top: 2px solid #dddddd;
		}
		.command_text {
		  font-family:Courier;
		  font-weight:bold;
		  font-size:large;
		}
		.firstHeading {
		  font-style: italic;	
		}
		.mw-headline {
		  color: #c01010;	
		}
		</style>
		<!-- Date: 2014-03-11 -->
	</head>
	<body>
		<section id='content'>
			<div id='bodyContent'>
				<h1 id='firstHeading' class='firstHeading'>
					Neutron/LBaaS/F5BigIP - Multi-Tenant Plugin Version 1.0-1 Release
				</h1>
				<hr/>
				<ul>
					<li><a href='#Supported_Neutron_Releases'>Supported Neutron Releases</a></li>
					<li><a href='LBaaS_scheduling'>Scheduling LBaaS Deployments</a></li>
					<li><a href='#Installation_of_the_Type_Driver'>Installation of the Plugin Side Type Driver</a></li>
					<li><a href='#Neutron_Server_Plugin_Settings'>Neutron Server Plugin Settings</a></li>
					<li><a href='#Installing_and_Running_a_LBaaS_Agent_Processes'>Installing and Running a LBaaS Agent Processes</a></li>
					<li><a href='#iControl_Driver_Supported_TMOS_Versions'>iControl Driver Supported TMOS Versions</a></li>					
					<li><a href='#Supported_Neutron_Network_Topologies'>Supported Neutron Network Topologies</a></li>
					<li><a href='#Multitenancy_Mapping'>OpenStack and TMOS Multitenancy</a></li>
					<li><a href='#Supported_TMOS_High_Availability_Modes'>Supported TMOS High Availability Modes</a></li>
					<li><a href='#Preparing_Your_TMOS_Device_for_LBaaS'>Preparing Your TMOS Device for LBaaS</a></li>
					<li><a href='#Troubleshooting_Issues'>Troubleshooting Issues</a></li>
				</ul>
				
				<h2><span class='mw-headline' id='Supported_Neutron_Releases'>Supported Neutron Releases</span></h2>
				<p>
				The f5 LBaaS plugin driver, as well as the f5 LBaaS agent with iControl driver, support the OpenStack 
				Neutron Havana release. There are no alternations made to the community LBaaS database schema, nor any 
				additional database tables generated. The stock community LoadBalancerPlugin class is used as the service
				plugin.  Any discovered issues with the community LoadBalancerPlugin class and the associated LBaaS database
				model will need to be monitored and resolved through the OpenStack Neutron community.
				</p>
				
				<p>
				For deployment modes which include tenant tunnels, Neutron <b>MUST</b> be using the ML2 core plugin, with the associated 
				Neutron ML2 types and data model classes. I.E.:
				</p>
				<p>
				<span class='command_text'>core_plugin = neutron.plugins.ml2.plugin.Ml2Plugin</span>
				</p>
				<p>
				Using the standard community ML2 plugin and type drivers will meet the f5 LBaaS plugin requirements.
				</p>
				<p>
				When Neutron networks are discovered by the plugin that fall into the classification of a Pool, Vip, or Member object, they must include the Neutron
				providernet extension data. The providernet extension adds attributes to Neutron networks allowing
				other services, such as the LBaaS service, to discern appropriate L2 network settings, allowing Neutron to connect appropriately. The 
				f5 LBaaS plugin uses these attributes to provision L2 connectivity on TMOS devices matching the L2 connectivity 
				for the appropriate Neutron networks. If the providernet extension data is not present on Neutron networks,
				proper L2 isolation and tenancy can not be provisioned on the TMOS devices. 
				</p>				
				<p>
				You can determine if your Neutron networks support this extension by showing the
				details for a Neutron network and checking for the following attributes, such as:
				</p>
				<p>
				<pre>
# neutron net-show Provider-VLAN-62
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 07f92400-4bb6-4ebc-9b5e-eb8ffcd5b34c |
| name                      | Provider-VLAN-62                     |
| <b>provider:network_type</b>      | vlan                                 |
| <b>provider:physical_network</b>  | ph-eth3                              |
| <b>provider:segmentation_id</b>   | 62                                   |
| router:external           | False                                |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   | a89aa39e-3a8e-4f2f-9b57-45aa052b87bf |
| tenant_id                 | 3aef8f59a43943359932300f634513b3     |
+---------------------------+--------------------------------------+				
				</pre>
				</p>
				<p>
				Typically only older style, monolithic Neutron core drivers will be a concern. ML2 core drivers should typically support
				providernet extensions without any additional configuration. 
				</p>
					
				<h2><span class='mw-headline' id='LBaaS_scheduling'>Scheduling LBaaS Deployments</span></h2>
										
				<p>
				The f5 LBaaS plugin driver schedules LBaaS requests across one or multiple f5 LBaaS agent processes. 
				OpenStack RPC queues are used to spread requests out to one or multiple f5 LBaaS agent processes. The 
				f5 agents register and use RPC queues to call back into the f5 LBaaS plugin driver to update 
				LBaaS object status and allocate Neutron ports and IP addresses as necessary.
				</p>
				
				<p>
				<img src='./images/plugin_agent.png' width='750px'>
				</p>
				
				<p>
				F5 LBaaS agents can be run on any host which has the appropriate Neutron python libraries installed. This includes 
				the controller or network nodes.  Alternatively, a host can be dedicated to service agents. Multiple f5 LBaaS
				agent processes can run on the same host simultaneously. 	
				</p>
				<p>
				F5 LBaaS agent processes are mapped directly to TMOS Device Service Groups. As a result there will always be one active 
				f5 LBaaS agent per TMOS Device Service Group.	
				</p>
				<p>
				By default the f5 LBaaS plugin uses an agent scheduler which will keep all LBaaS Pools associated with the same tenant on the same
				TMOS Device Service Group.  This aids in scaling by keeping all network objects associated with an OpenStack tenant on 
				the same TMOS Device Service Group instead of requiring them to be provisioned on all possible TMOS Device Service
				Groups in your cloud deployment.
				</p>
				<p>
				<img src='./images/agent_to_dsg.png' width='750px'>
				</p>
				<p>
				Other scheduling from pool to agent is possible with the use of a custom scheduler class. Please reference the plugin configuration setting for
				<span class='command_text'>f5_loadbalancer_pool_scheduler_driver</span> in the <span class='command_text'>neutron.conf</span> 
				configuration file in following section. Only the tenant affinity scheduler, described above, is included with the f5 LBaaS plugin.
				</p>
				<p>
				F5 LBaaS agent processes use standard Neutron agent registration and state reporting. The health and details 
				of registered f5 LBaaS agents can be queried using typical Neutron agent commands.
				</p>
				<p>
                <pre>
# neutron agent-list
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
| id                                   | agent_type         | host                                                 | alive | admin_state_up |
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
| 034bddd0-0ac3-457a-9e2c-ed456dc2ad53 | Open vSwitch agent | sea-osp-cmp-001                                      | :-)   | True           |
| 17e447de-6589-4419-ac83-36ffb7e8b328 | Open vSwitch agent | sea-osp-cmp-002                                      | :-)   | True           |
| 301f6e21-a8f2-4f58-a4a3-38eabc0c2987 | Open vSwitch agent | sea-osp-net-001                                      | :-)   | True           |
| 5ecd96ab-d01e-4a64-92e8-9cd7caa8f25e | L3 agent           | sea-osp-net-001                                      | :-)   | True           |
| b50b8b21-0d0b-4776-a6ec-eeab61590f10 | DHCP agent         | sea-osp-net-001                                      | :-)   | True           |
| <b>fb7bff08-3e45-475a-a02a-8020a2e0763a | Loadbalancer agent | sea-osp-ctl-001:b1cbd354-c78f-572d-8492-f6be4f7ef0bd | :-)   | True</b>           |
+--------------------------------------+--------------------+------------------------------------------------------+-------+----------------+
                </pre>
                </p>	
                <p>
                F5 LBaaS agent processes support the configuration of static name value parameters which will get passed to the
                plugin every time it updates its status.  These custom values can be used to inventory the f5 LBaaS agents or 
                as input to custom agent schedulers. Please see the reference to the <span class='command_text'>static_agent_configuration_data</span> 
                entry in the agent <span class='command_text'>f5-bigip-lbaas-agent.ini</span> configuration in the following section.
                </p>
           
				<p>
				High availability for the f5 LBaaS agent process follows the same pattern as any OpenStack agent.  Please see the 
				OpenStack high availability guides for instructions on how to run active and standby agents in a high availability mode.
				</p>
				<p>
				<a href='http://docs.openstack.org/high-availability-guide/content/ha-using-active-passive.html'>OpenStack Active-Stanby Agent Guide</a>
				</p>
				<p>
				Please see the section below for the high availability modes supported for the TMOS devices themselves.	
				</p>
				
				<h2><span class='mw-headline' id='Installation_of_the_Type_Driver'>Installation of the Plugin Side Type Driver</span></h2>
				<p>
				The Neutron server plugin driver is distributed as a Debian installation package. It has been tested
				on Ubuntu Server 12.04 LTS. To install the Neutron server plugin driver, download the 
				<span class='command_text'>f5-lbaas-driver_1.0-1_all.deb</span> package to the host which runs your Neutron API server 
				process. To install run:
				</p>
				<p>
				<span class='command_text'>sudo dpkg -i f5-lbaas-driver_1.0-1_all.deb</span>	
				</p> 
				
				<h2><span class='mw-headline' id='Neutron_Server_Plugin_Settings'>Neutron Server Plugin Settings</span></h2>
				<p>
				There are two settings which must be set to enable Neutron LBaaS with the F5 LBaaS type driver. The services entry for 
				Neutron must include a reference to the default community LoadBalancerPlugin. The <span class='command_text'>services</span> 
				configuration entry in the default <span class='command_text'>neutron.conf</span> file contains a comma separated list 
				of python classes which implement extension plugin services. The following example would load the default community LoadBalancerPlugin.
				</p>
				<p>
					In the <span class='command_text'>neutron.conf</span> file:
				</p>
				<p>
					<span class='command_text'>service_plugins = neutron.services.loadbalancer.plugin.LoadBalancerPlugin</span>
				</p>
				<p> 
				Once the default community LoadBalancerPlugin plugin is loaded, service provider type drivers entries will be examined. Providing 
				the following service provider type driver entry for the <span class='command_text'>LOADBALANCER</span> plugin service will load 
				the f5 LBaaS type driver:
				</p>
				<p>
					<span class='command_text'>service_provider=LOADBALANCER:F5:neutron.services.loadbalancer.drivers.f5.plugin_driver.F5PluginDriver:default</span>
				</p> 
				<p>
				You must have at least one <span class='command_text'>default</span> entry for each plugin service. If the F5 plugin type driver
				is not the default, simply remove the <span class='command_text'>default</span> field from the above entry.
				</p>
				<p>
				In the default section of your <span class='command_text'>neutron.conf</span> file, the optional 
				<span class='command_text'>f5_loadbalancer_pool_scheduler_driver</span> variable can be set to the full python 
				class path for an alternative agent scheduler.  The default value for this setting,
				<span class='command_text'>neutron.services.loadbalancer.drivers.f5.agent_scheduler.TenantScheduler</span>,
				will cause the plugin to assign pools to agents with tenant affinity as described above.
				</p>
				
				
				<h2><span class='mw-headline' id='Installing_and_Running_a_LBaaS_Agent_Processes'>Installing and Running an LBaaS Agent Processes</span></h2>
				
				<p>
				The f5 LBaaS agent is distributed as a Debian installation package. It has been tested on Ubuntu Server 12.04 LTS. To install a single LBaaS agent, 
				first download the f5-bigip-lbaas-agent_1.0-1_all.deb package to the host which will run your agent process. Once downloaded, run:
				</p>
				
                <span class='command_text'>
				sudo dpkg -i f5-bigip-lbaas-agent_1.0-1_all.deb	
				</span>
				
				<p>
				On Ubuntu systems an upstart job will run as a result of the installation.  You should stop this service and configure your f5 LBaaS agent:
				</p>

                <span class='command_text'>
				sudo service f5-bigip-lbaas-agent stop	
				</span>
				
				<p>
				The default configuration file for the f5 LBaaS agent is located at <span class='command_text'>/etc/neutron/f5-bigip-lbaas-agent.ini</span>.
				Below is a consideration of each setting in that configuration file.
				</p>
				
				<p>
				<table border='1' width='1024px' cellpadding='2'>
					<tbody>
					<tr><th nowrap>Setting</th><th nowrap>Allowed Values</th><th nowrap>Default Value</th><th nowrap>Description</th></tr>
					<tr><td nowrap>debug</td><td>True or False</td><td>False</td><td>Should the agent create verbose debug logging in /var/log/neutron/f5-bigip-lbaas-agent.log</td></tr>
					<tr><td nowrap>periodic_interval</td><td>integer number of seconds</td><td>10</td><td>How often should the get_stats, and save configurations be considered. (not run, but considered)</td></tr>
					
					<tr><td nowrap>service_resync_interval</td><td>integer number of seconds<td>500</td><td>How often the agent pulls known configurations with neutron and cleans up orphaned configurations on the BIG-IPs.</td></tr>
					<tr><td nowrap>use_namespace</td><td>True or False</td><td>True</td><td>Should all Neutron non-shared subnet addresses be assigned to route domains</td></tr>
					<tr><td nowrap>static_agent_configuration_data</td><td>single or comma separated list</td><td>None</td><td>This list of name:value attribues will be sent as part of the agent configuration data to the plugin.</td></tr>				    
					<tr><td nowrap>environment_prefix</td><td>String prefix</td><td>'' (blank string)</td><td>This is the prefix this agent will use for all BIG-IP objects. Default is a blank string which will result in BIG-IP objects prefixed with uuid_.</td></tr>
					<tr><td nowrap>f5_device_type</td><td>external</td><td>external</td><td>This determines the type of automatica device onboarding.  Only external, is acceptable for this release.</td></tr>
				    <tr><td nowrap>f5_ha_type</td><td>standalone, pair, or scalen</td><td>pair</td><td>This determines the way LBaaS will be deployed for HA</td></tr>
				    <tr><td nowrap>f5_sync_mode</td><td>auto_sync or replication</td><td>replication</td><td>This determines the way LBaaS will be provisioned. For auto_sync, only a single TMOS device will be provisioned and DSG syncing used to replicate the configuration objects.  For replication, each device will be provisioned independantly. The recommended mode is replication.  It is faster for large configurations.</td></tr>
				    <tr><td nowrap>f5_external_physical_mappings</td><td>comma separated list</td><td>default:1.1:True</td><td>Determines how Neutron VLAN networks are created. The format for each mapp entry is 
				    	    [provider:physical_network]:[TMOS interface or trunk name]:[Boolean to tag the VLAN ]. One entry with 'default' as it initial entry should be present to allow for cases where 
				    	    the provider:physical_network does not match any other specified mapping.</td></tr>
				    <tr><td nowrap>f5_vtep_folder</td><td>TMOS folder name</td><td>Common</td><td>For tenant tunneling, this determines the TMOS folder to discover the VTEP interface.</td></tr>
				    <tr><td nowrap>f5_vtep_selfip_name</td><td>TMOS Self IP name</td><td>vtep</td><td>For tennant tunneling, this determines the pre-provisioned VTEP Self IP name.</td></tr>
				    <tr><td nowrap>advertised_tunnel_types</td><td>gre<br/>vxlan<br/>gre,vxlan</td><td>gre,vxlan</td><td>Specifies the tunnel types the agent advertises via RPC to other VTEPs. This should match your ML2 allowed network_typs. Default is to advertise both gre and vxlan.</td></tr>
				    <tr><td nowrap>f5_global_routed_mode</td><td>True or False</td><td>False</td><td>If set, only L4+ objects will be provisioned.  All necessary L2 and L3 routes will be assumed pre-provisioned. SNAT will be truned to AutoMAP on all VIPs, and all L2 and L3 tenancy isolation will not be provisioned.</td></tr>
				    <tr><td nowrap>f5_snat_mode</td><td>True or False</td><td>True</td><td>For L3 forwarding, should SNAT be used. If set to False, the LBaaS device attempt to become the default gateway for the Neutron subnet of Member objects.</td></tr>
				    <tr><td>f5_snat_addresses_per_subnet</td><td>integer number of SNAT addresses</td><td>1</td><td>If set to 0(zero), AutoMAP SNAT will be used.  If set to a positive integer, that number of SNAT addresses will
				    	     be created per Neutron subnet of Members, per active TMOS device in an HA mode.</td></tr>
				    <tr><td nowrap>f5_route_domain_strictness</td><td>True or False</td><td>False</td><td>If set to True, tenant route domain isolation will be set to strict and VIPs and Members must be placed in the same route domain. This would mean that all Neutron networks with the shared attribute can not be used for VIPs or Members.</td></tr>
				    <tr><td nowrap>f5_common_external_networks</td><td>True or False</td><td>False</td><td>If set to True, any VIP address or member address associated with a Neutron network with router:external set to True will be created in the Common parition and in the default route domain.</td></tr>  
				    <tr><td nowrap>common_network_ids</td><td>comma separated list</td><td>None</td><td>List in [vlan]:[neutron network UUID] format. All defined network UUIDs defined will exempt services from creating new VLANs, but will use the /Common/[vlan] specified instead.</td></tr>
				    <tr><td>f5_bigip_lbaas_device_driver</td><td>python class name of the device driver to use</td><td>neutron.services.loadbalancer.drivers.f5.bigip.icontrol_driver.iControlDriver</td><td>Only the neutron.services.loadbalancer.drivers.f5.bigip.icontrol_driver.iControlDriver is supplied.</td></tr>
				    <tr><td>icontrol_hostname</td><td>either a single entry or a comma separated list iControl endpoint IP address or FQDNs </td><td>192.168.1.245</td><td>The iControl endpoint to connect to for this agent. If a comma separated list is given, you must include each device in your DSG. If only one address is given, the other device's management address will be learned from the DSG and must be reachable from the LBaaS agent.</td></tr>
				    <tr><td>icontrol_username</d><td>valid username for the TMOS device</td><td>admin</td><td>The iControl endpoint username for this agent</td></tr>
				    <tr><td>icontrol_password</d><td>valid password for the TMOS device</td><td>admin</td><td>The iControl endpoint password for this agent</td></tr>
				    <tr><td>icontrol_connection_retry_interval</td><td>integer number of seconds</td><td>10</td><td>How often to attempt to reconnect if iControl connection fails</td></tr>
				    </tbody>
				</table>
				</p>
				<p>
				<h3>Starting Multiple Agents on the Same Host</h3>
				</p>
				<p>
				In order to start more than one agent on the same host, several alterations need to be made for each additional agent instance: 
				<ol>
				<li>Create a unique configuration file for each agent, using <span class='command_text'>/etc/neutron/f5-bigip-lbaas-agent.ini</span> as a template.</li>
				<li>Create additional upstart conf files in /etc/init using <span class='command_text'>/etc/init/f5-bigip-lbaas-agent.conf</span> as a template.
	                The <span class='command_text'>exec</span> line in each upstart conf file should be customized to point to each agent's unique configuration file and a unique agent log file.
	            </li>
				<li>Start each agent using the name of its unique upstart conf file.</li>
				</ol>
				</p>
	
				
				<h2><span class='mw-headline' id='iControl_Driver_Supported_TMOS_Versions'>iControl Driver Supported TMOS Versions</span></h2>
				<p>
				The agent processes themselves include orchestration methods which are available on TMOS 11.5 and greater. The agents will check for this version of TMOS on all connected devices. If an older version of TMOS is detected, the agents will not register with Neutron. 
				</p>
				<h2><span class='mw-headline' id='Supported_Neutron_Network_Topologies'>Supported Neutron Network Topologies</span></h2>
				<p>
				The F5 iControl agent driver supports the following network topologies with either hardware appliances or TMOS virtual editions:
				</p>
				<ul>
					<li> 
					<p>
					Global routed mode where all VIPs are assumed routable from clients and all Members are assumed routable from the
					TMOS devices themselves.  All L2 and L3 objects, including routes, must be pre-provisioned on the TMOS Device Service
					Group prior to LBaaS provisioning.
					</p>
					<p>
					<table>
					<tr><th>Topology</th><th>f5-bigip-lbaas-agent.ini settings</th></tr>
					<tr>
					<td><img src='./images/global_routed_mode.png'></td>
					<td>
					<p>
					<span class='command_text'>
					f5_global_routed_mode = True
					</span>
					</p>
					</td>
					</tr>
					</table>
					</p>
					<p>
					Global routed mode uses TMOS AutoMap SNAT for all VIPs.  Because no explicit SNAT pools are being defined, sufficient 
					Self IP addresses should be created to handle connection loads. 
					</p>
					<p>
					<b><i>Warning:</i></b> In global routed mode, because all access to and from the TMOS devices is assumed globally routed, 
					there is no segregation between tenant services on the TMOS devices themselves. Overlapping IP address spaces for 
					tenant objects is likewise not available.
					</p>     
				    </li>
					
					<li> 
					<p>One-Arm mode where VIP and Members can be provisioned from the same Neutron subnet.</p>
					<p>
					<table>
					<tr><th>Topology</th><th>f5-bigip-lbaas-agent.ini settings</th></tr>
					<tr>
					<td><img src='./images/one_arm.png'></td>
					<td>
					<p>
					<span class='command_text'>
					f5_global_routed_mode = False</br>
					f5_snat_mode = True</br>
					</span>
					</p>
					<p>
					</br>
					optional settings:</br>
					</p>
					<p>
					<span class='command_text'>
					f5_snat_addresses_per_subnet = n</br>
					</span>
					</p>
					<p>
					where if n is 0, the virtual server will use AutoMap SNAT.  If n is > 0, n number of SNAT addresses will be allocated
					from the Member subnet per active traffic group.
					</p>
					</td>
					</tr>
					</table>
					</p>
					</li>
					
					
					
					<li> 
					<p>Multiple-Arm mode where VIP and Members are provisioned from different Neutron subnets.</p>
					<p>
					<table>
					<tr><th>Topology</th><th>f5-bigip-lbaas-agent.ini settings</th></tr>
					<tr>
					<td><img src='./images/multiarm_snat.png'></td>
					<td>
					<p>
					<span class='command_text'>
					f5_global_routed_mode = False</br>
					f5_snat_mode = True</br>
					</span>
					</p>
					<p>
					</br>
					optional settings:</br>
					</p>
					<p>
					<span class='command_text'>
					f5_snat_addresses_per_subnet = n</br>
					</span>
					</p>
					<p>
					where if n is 0, the virtual server will use AutoMap SNAT.  If n is > 0, n number of SNAT addresses will be allocated
					from the Member subnet per active traffic group.
					</p>
					</td>
					</tr>
					</table>
					</p>
					</li>
					
					
					
					<li>
					<p>Gateway routed mode where attemps will be made to create a default gateway forwarding service on the TMOS Device
					   Service Group for Member Neutron subnets</p>
					<p>
					<table>
					<tr><th>Topology</th><th>f5-bigip-lbaas-agent.ini settings</th></tr>
					<tr>
					<td><img src='./images/routed_mode.png'></td>
					<td>
					<p>
					<span class='command_text'>
					f5_global_routed_mode = False</br>
					f5_snat_mode = False</br>
					</span>
					</p>
					</td>
					</tr>
					</table>
					</p>
					     
					</li>
				</ul>
				

				
				<p>
				For the Neutron network topologies requiring dynamic L2 and L3 provisioning of the TMOS devices, which includes all
				network topologies except global routed mode, the f5 LBaaS iControl driver supports:
				</p>
				<ul>
				    <li>Provider VLANs - VLANs defined by the admin tenant and shared</li>
				    <li>Tenant VLANs - VLANs defined by the admin tenant for other tenants or by the tenant themselves</li>
				    <li>Tenant GRE Tunnels - GRE networks defined by the tenant</li>
				    <li>Tenant VxLAN Tunnels - VxLAN networks defined by the tenant</li>
				</ul>
				<h3>VLANs</h3>
				<p>
				For VLAN connectivity, the f5 TMOS devices use a mapping between the Neutron network <span class='command_text'>provider:physical_network</span>
				attribute and TMM interface names. This is analogous to the Open vSwitch agents mapping between the Neutron network <span class='command_text'>provider:physical_network</span>
				and their interface bridge name.  The mapping is created in the <span class='command_text'>f5-bigip-lbaas-agent.ini</span> configuration file using in the 
				<span class='command_text'>f5_external_physical_mappings</span> setting.  The name of the <span class='command_text'>provider:physical_network</span> entries can be added
				to a comma separated list with mappings to the TMM interface or LAG trunk name, and a boolean attribute to specify if 802.1q tagging will be applied. An example which
				would map the <span class='command_text'>provider:physical_network</span> containing 'ph-eth3' to TMM interface 1.1 with 802.1q tagging would look like this:
				</p>
				<ul>
				<span class='command_text'>f5_external_physical_mappings = ph-eth3:1.1:True</span>
				</ul>
				<p>
				A default mapping should be included for cases where the <span class='command_text'>provider:physical_network</span> does not match any configuration settings. A default
				mapping simply uses the word <span class='command_text'>default</span> instead of a known <span class='command_text'>provider:physical_network</span> attribute.
				</p>
				<p>
				An example that would include the previously illustrated mapping, a default mapping, and LAG trunk mapping, might look like this:
				</p>
				<ul>
				<span class='command_text'>f5_external_physical_mappings = default:1.1:True, ph-eth3:1.1:True, ph-eth4:lag-trunk-1:True</span>
				</ul>
				<h3>Tunnels</h3>
				<p>
				<H2>
				<span class='mw-headline'>
				Warning - To use GRE or VxLAN tunnels with versions less then TMOS 11.6, you must apply an engineering hotfix.
				The minimum recommended release for use with tunnels is TMOS 11.5.1 with engineering hotfix BIGIP-11.5.1.2.38.121-HF2-ENG.
				To obtain this hotfix, please open a support case on your TMOS device and request it. 
				</span>
				</H2>
				</p>
				<p>
				For GRE and VxLAN tunnels, the f5 TMOS devices expect to communicate with Open vSwitch VTEPs. The VTEP addresses
				for Open vSwitch VTEPs are learned from their registered Neutron agent configurations
				<span class='command_text'>tunneling_ip</span> attribute. I.E.:
				</p>
				<p>
				
				<pre>
# neutron agent-show 034bddd0-0ac3-457a-9e2c-ed456dc2ad53
+---------------------+--------------------------------------+
| Field               | Value                                |
+---------------------+--------------------------------------+
| admin_state_up      | True                                 |
| agent_type          | Open vSwitch agent                   |
| alive               | True                                 |
| binary              | neutron-openvswitch-agent            |
| configurations      | {                                    |
|                     |      "tunnel_types": [               |
|                     |           "gre"                      |
|                     |      ],                              |
|                     |      "<b>tunneling_ip</b>": "10.1.0.35",    |
|                     |      "bridge_mappings": {            |
|                     |           "ph-eth3": "br-eth3"       |
|                     |      },                              |
|                     |      "l2_population": true,          |
|                     |      "devices": 4                    |
|                     | }                                    |
| created_at          | 2013-11-15 05:00:23                  |
| description         |                                      |
| heartbeat_timestamp | 2014-04-22 16:58:21                  |
| host                | sea-osp-cmp-001                      |
| id                  | 034bddd0-0ac3-457a-9e2c-ed456dc2ad53 |
| started_at          | 2014-04-17 22:39:30                  |
| topic               | N/A                                  |
+---------------------+--------------------------------------+
				</pre>
				</p>
				<p>				
				The ML2 L2 Population service is supported by the f5 LBaaS iControl driver, such that only Open vSwitch agents hosting
				Members will have overlay tunnels built to them for Member IP access.  When the ML2 L2 Population service is used,
				static ARP entries will be created on the TMOS devices to remove the need for them to send ARP broadcast (flooding) 
				across the tunnels to learn the location of Members. In order to support this, the ML2 port binding extensions and 
				segmentation models must be present.  The port binding extensions and segmentation model are defined by default
				with the community ML2 core plugin and Open vSwitch agents on the compute nodes.  
				</p>
				<p>
				When VIPs are placed on tenant overlay networks, the f5 LBaaS agent will send tunnel update RPC messages to the Open vSwitch
				agents informing them of TMOS device VTEPs. This enables tenant guest virtual machines or network node services to interact
				with the TMOS provisioned VIPs across overlay networks. An f5 LBaaS agent's connected TMOS VTEP addresses are placed in
				the agent's configurations and reported to Neutron. The VTEP addresses are listed as <span class='command_text'>tunneling_ips</span>.
				</p>
				<p>
				<pre>
				
# neutron agent-show 0b7bff08-3e45-475a-a02a-8020a2e0763a
+---------------------+------------------------------------------------------+
| Field               | Value                                                |
+---------------------+------------------------------------------------------+
| admin_state_up      | True                                                 |
| agent_type          | Loadbalancer agent                                   |
| alive               | True                                                 |
| binary              | f5-bigip-lbaas-agent                                 |
| configurations      | {                                                    |
|                     |      "icontrol_endpoints": {                         |
|                     |           "10.0.40.122": {                           |
|                     |                "device_name": "bigip-122.f5se.com",  |
|                     |                "platform": "3900",                   |
|                     |                "version": "BIG-IP_v11.5.0",          |
|                     |                "serial_number": "f5-htfx-njih"       |
|                     |           },                                         |
|                     |           "10.0.40.121": {                           |
|                     |                "device_name": "bigip-121.f5se.com",  |
|                     |                "platform": "3900",                   |
|                     |                "version": "BIG-IP_v11.5.0",          |
|                     |                "serial_number": "f5-qprg-nkdl"       |
|                     |           }                                          |
|                     |      },                                              |
|                     |      "agent_env": "dev",                             |
|                     |      "vpc_zone": "east",                             |
|                     |      "<b>tunneling_ips</b>": [                              |
|                     |           "10.1.0.122",                              |
|                     |           "10.1.0.121"                               |
|                     |      ],                                              |
|                     |      "services": 23,                                  |
|                     |      "tunnel_types": [                               |
|                     |           "vxlan",                                   |
|                     |           "gre"                                      |
|                     |      ],                                              |
|                     |      "bridge_mappings": {                            |
|                     |           "default": "1.1",                          |
|                     |           "ph-eth3": "1.1"                           |
|                     |      },                                              |
|                     |      "global_routed_mode": false                     |
|                     | }                                                    |
| created_at          | 2014-04-02 02:03:37                                  |
| description         |                                                      |
| heartbeat_timestamp | 2014-04-22 17:16:25                                  |
| host                | sea-osp-ctl-001:b1cbd354-c78f-572d-8492-f6be4f7ef0bd |
| id                  | 0b7bff08-3e45-475a-a02a-8020a2e0763a                 |
| started_at          | 2014-04-22 16:25:00                                  |
| topic               | f5_lbaas_process_on_agent                            |
+---------------------+------------------------------------------------------+
				
				
				</pre>
				</p>
				<p>
				
				<h2><span class='mw-headline' id='Multitenancy_Mapping'>OpenStack and TMOS Multinenancy</span></h2>
		        <p>
				By default, all TMOS objects are created in administrative partitions associated with the OpenStack <span class='command_text'>tenant_id</span>
				for the pool.  		
				</p>
				<p>
				If the <span class='command_text'>f5-bigip-lbaas-agent.ini</span> configuration file setting for <span class='command_text'>use_namespaces</span>
				is set to <span class='command_text'>True</span>, and it is not configured for global routed mode, each tenant will also have a TMOS route domain
				created, providing segmentation for IP address spaces between tenants.  
				</p>
				<p>
				If an associated Neutron network for a VIP or Member is marked as <span class='command_text'>shared=True</span>, and the f5 LBaaS 
				agent is not in global routed mode, all associated L2 and L3 objects will be created in the <span class='command_text'>/Common</span> 
				administrative partition and associated with route domain 0 (zero) on all TMOS devices. 
				</p>		
				<p>
				<img src='./images/tenancy_mapping.png'>
				</p>
				
				<h2><span class='mw-headline' id='Supported_TMOS_High_Availability_Modes'>Supported TMOS High Availability Modes</span></h2>
				
				<p>
				The F5 iControl agent driver supports:
				</p>
				<ul>
					<li> Standalone mode - Single TMOS device providing no High Availability</li>
					<li> Pair mode - Active / Standby TMOS devices</li>
					<li> ScaleN mode - Multiple Active TMOS devices, up to 4 (four) devices</li>
				</ul>	
				<p>
				Every TMOS device, hardware or virtual edition, must be on-boarded, or pre-provisioned, before taking LBaaS requests. 
				If VLANs are used with the TMOS device, these devices are expected to support tagged VLAN frames on the TMOS interfaces if 
				provisioned to do so.	
				</p>
				
				<p>
				Note: TMOS vCMP guests are currently not supported by the f5 LBaaS Plugin.
				</p>
				
				<h2><span class='mw-headline' id='Preparing_Your_TMOS_Device_for_LBaaS'>Preparing Your TMOS Device for LBaaS</span></h2>
				<p>
				TMOS devices must be on-boarded to support their appropriate high availability mode. All of the following on-boarding steps
				can be automated using iControl. Below is a listing of all the manual CLI commands needed to complete the TMOS device on-boarding. 	
				</p>
				
				<h3>Standalone High Availability Mode</h3>
				<p>
				The TMOS device must be licensed to support Local Traffic (LTM). The TMOS device should have Local Traffic (LTM) provisioned as Nominal.
				</p>
				
				<p>
				To license a TMOS device via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>SOAPClientClient --interactive &lt;license_basekey&gt;</span>
				</ul>
				<p>
				To provision the Local Traffic (LTM) to nominal via the CLI, run:	
				</p>
				<ul>
				    <span class='command_text'>tmsh modify sys provision ltm level nominal</span></br>
				    <span class='command_text'>tmsh modify sys db provision.extramb value 500</span>
				</ul>
				
				<p>
				For Standalone Mode, SNAT addresses will get created on, and VIPs will get scheduled to, the default traffic-group-local-only non-floating traffic group.
				</p>
				
				
				
				
				
			
				<h3>Pair High Availability Mode</h3>
				<p>
				Both TMOS devices must be licensed to support Local Traffic (LTM). The TMOS devices should have Local Traffic (LTM) provisioned as Nominal.
				</p>
				<p>
				To license a TMOS device via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>SOAPClientClient --interactive &lt;license_basekey&gt;</span>
				</ul>
				<p>
				To provision the Local Traffic (LTM) to nominal via the CLI, run:	
				</p>
				<ul>
				    <span class='command_text'>tmsh modify sys provision ltm level nominal</span></br>
				    <span class='command_text'>tmsh modify sys db provision.extramb value 500</span>
				</ul>
				<p>
				Both TMOS devices must have their Network Time Protocol servers set to the same NTP server.	Both TMOS devices should also have their timezone
				set to the same timezone.
				</p>
				<p>
				To set a TMOS device NTP server via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh modify sys ntp servers replace-all-with { &lt;ip_address_ntp_1&gt; &lt;ip_address_ntp_2&gt; }</span>
				</ul>
				<p>
				To set a TMOS device timezone via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify sys ntp timezone UTC</span>
				</ul>
				
				<p>
				Both TMOS devices must share a common VLAN for HA management communication. Both devices will require a local IP address to communicate 
				cluster heartbeat and synchronization information. 
				</p>
				
				<p>
				To provision a TMOS device HA VLAN via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh create net vlan HA interfaces add { &lt;TMOS_interface_name&gt; } description HA</span>
				</ul>
				<p>
				To provision a TMOS device IP address on the HA VLAN via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh create net self HA &lt;ip_address/cidr&gt; allow-service default vlan HA</span>
				</ul>
                
				<p>
				Both TMOS devices must have their configuration sync interfaces, connection mirroring, and failover addresses defined. 
				</p>
				<p>
				To define a TMOS device configuration sync address via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh modify cm device &lt;device_name&gt; configsync-ip &lt;ha_ip_address&gt;</span>
				</ul>
				<p>
				To define a TMOS device connection mirroring address via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm device &lt;device_name&gt; mirror-ip &lt;ha_ip_address&gt;</span>
                </ul>
                <p>
				To define a TMOS device failover address via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm device &lt;device_name&gt; unicast-address { { ip &lt;ha_ip_address&gt; port 1026 } { ip &lt;device_mgmt_ip_address&gt; port 1026 } }</span>
				</ul>
				
				<p>
				Both TMOS devices must have distinct device names before they can be placed in a Device Service Group. Once they have distinct device names, 
				a trust must be built between the devices.
				</p>
				
				<p>
				To reset TMOS device name via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh mv cm device &lt;original_name&gt; &lt;new_name&gt;</span>
				</ul>
				<p>
				To add a trusted peer <i>from your primary TMOS device</i> via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm trust-domain /Common/Root ca-devices add { &lt;peer_mgmt_ip_address&gt; } name &lt;remove_device_name&gt; username &lt;remote_device_admin_username&gt; password &lt;remote_device_admin_password&gt;</span>
				</ul>
				
				<p>
				Both TMOS devices must be placed into a sync failover Device Service Group to allow active/standby failover. An initial sync must be run on the new sync failover
				Device Service Group.
				</p>
				<p>
				To create the sync failover Device Service Group <i>from your primary TMOS device</i> via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh create cm device-group &lt;device_group_name&gt; devices add { &lt;primary_device_name&gt; &lt;remote_device_name&gt; } type sync-failover auto-sync enabled network-failover enabled</span>
				</ul>
				To force an initial sync for the sync failover Device Service Group <i>from your primary TMOS device</i> via the CLI, run:
				<ul>
				    <span class='command_text'>tmsh run cm config-sync to-group &lt;device_group_name&gt;</span>
				</ul>
				
				<p>
				For Pair High Availability Mode, SNAT addresses will get created on, and VIPs will get scheduled to, the default traffic-group-1 floating traffic group.
				</p>
				
				
				
				
	
				
	
                <h3>ScaleN High Availability Mode</h3>
				<p>
				All TMOS devices in your ScaleN group must be licensed to support Local Traffic (LTM). The TMOS device should have Local Traffic (LTM) provisioned as Nominal.
				</p>
				
				<p>
				To license a TMOS device via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>SOAPClientClient --interactive &lt;license_basekey&gt;</span>
				</ul>
				<p>
				To provision the Local Traffic (LTM) to nominal via the CLI, run:	
				</p>
				<ul>
				    <span class='command_text'>tmsh modify sys provision ltm level nominal</span></br>
				    <span class='command_text'>tmsh modify sys db provision.extramb value 500</span>
				</ul>

				<p>
				All TMOS devices must have their Network Time Protocol servers set to the same NTP server.	Both TMOS devices should also have their timezone
				set to the same timezone.
				</p>
				<p>
				To set a TMOS device NTP server via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh modify sys ntp servers replace-all-with { &lt;ip_address_ntp_1&gt; &lt;ip_address_ntp_2&gt; }</span>
				</ul>
				<p>
				To set a TMOS device timezone via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify sys ntp timezone UTC</span>
				</ul>
				
				<p>
				All TMOS devices must share a common VLAN for HA management communication. All devices will require a local IP address to communicate 
				cluster heartbeat and synchronization information. 
				</p>
				
				<p>
				To provision a TMOS device HA VLAN via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh create net vlan HA interfaces add { &lt;TMOS_interface_name&gt; } description HA</span>
				</ul>
				<p>
				To provision a TMOS device IP address on the HA VLAN via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh create net self HA &lt;ip_address/cidr&gt; allow-service default vlan HA</span>
				</ul>
                
				<p>
				All TMOS devices must have their configuration sync interfaces, connection mirroring, and failover addresses defined. 
				</p>
				<p>
				To define a TMOS device configuration sync address via the CLI, run:
				</p>
				<ul>
					<span class='command_text'>tmsh modify cm device &lt;device_name&gt; configsync-ip &lt;ha_ip_address&gt;</span>
				</ul>
				<p>
				To define a TMOS device connection mirroring address via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm device &lt;device_name&gt; mirror-ip &lt;ha_ip_address&gt;</span>
                </ul>
                <p>
				To define a TMOS device failover address via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm device &lt;device_name&gt; unicast-address { { ip &lt;ha_ip_address&gt; port 1026 } { ip &lt;device_mgmt_ip_address&gt; port 1026 } }</span>
				</ul>
				
				<p>
				All TMOS devices must have distinct device names before they can be placed in a Device Service Group. Once they have distinct device names, 
				a trust must be built between the devices.
				</p>
				
				<p>
				To reset TMOS device name via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh mv cm device &lt;original_name&gt; &lt;new_name&gt;</span>
				</ul>
				<p>
				To add a trusted peer <i>from your primary TMOS device</i> for each peer device via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh modify cm trust-domain /Common/Root ca-devices add { &lt;peer_mgmt_ip_address&gt; } name &lt;remove_device_name&gt; username &lt;remote_device_admin_username&gt; password &lt;remote_device_admin_password&gt;</span>
				</ul>
				
				<p>
				All TMOS devices must be placed into a sync failover Device Service Group to allow active/standby failover. An initial sync must be run on the new sync failover
				Device Service Group.
				</p>
				<p>
				To create the sync failover Device Service Group <i>from your primary TMOS device</i> via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>tmsh create cm device-group &lt;device_group_name&gt; devices add { &lt;primary_device_name&gt; &lt;remote_device_name&gt; .. &lt;additional_remote_device_name&gt; } type sync-failover auto-sync enabled network-failover enabled</span>
				</ul>
				To force an initial sync for the sync failover Device Service Group <i>from your primary TMOS device</i> via the CLI, run:
				<ul>
				    <span class='command_text'>tmsh run cm config-sync to-group &lt;device_group_name&gt;</span>
				</ul>
				
				<p>
				To enable failover between the active devices you must create traffic groups.  The scheme you choose to create traffic groups will depend on your failover
				group design.  The iControl driver in ScaleN failover mode will schedule VIPs and SNAT addresses on traffic groups which are not named 
				traffic-group-1 or traffic-group-local-only.  It is required that you create unique named traffic groups for you failover pattern which 
				are associated with an appropriate HA failover order. The iControl driver will schedule the VIP traffic group placement based on which 
				unique named traffic group has least number of VIPs associated with it. 
				</p>
				<p>
				To create one of multiple traffic groups with explicit HA order <i>from your primary TMOS device</i> via the CLI, run:
				</p>
				<ul>
				    <span class='command_text'>
				    	tmsh create cm traffic-group &lt;traffic_group_name&gt; default-device &lt;device_name&gt; auto-failback-enabled true ha-order { &lt;secondary_device_name&gt; .. &lt;additional_device_name&gt;  }
				    </span>
				</ul>
					
			    <h3>Tenant Tunnel VTEP Self IPs</h3>
			    <p>
			    In order to terminate GRE or VxLAN tenant tunnels each TMOS device must have VTEP non-floating Self IP addresses configured.  The folder and name of the
			    VTEP interface must correspond to the <span class='command_text'>f5_vtep_folder</span> and <span class='command_text'>f5_vtep_selfip_name</span> settings 
			    for the agent. The VTEP Self IPs must be able to route IP packets to the rest of the Open vSwitch VTEP addresses.
			    </p>		
			    <p>
			    To create a VTEP Self IP via the CLI,
			    <ul>
			       <span class='command_text'>tmsh create net self [vtep selfip name] <ip_address/cidr> allow-service default vlan [vtep VLAN name]</span>
			    </ul>
			    </p> 
			    <p>
			    A connectivity test should be made from each TMOS device to Open vSwitch VTEP address to ensure L3 routing is functioning properly.
			    </p>
					
					

				<h2><span class='mw-headline' id='Troubleshooting_Issues'>Troubleshooting Issues</span></h2>
				
				<p>
				To troubleshoot problems with the F5 LBaaS Type driver or an agent process, set the global Neutron setting and agent process <span class='command_text'>debug</span> 
				setting to <span class='command_text'>True</span>. Extensive logging will then appear in the <span class='command_text'>neutron-server</span> and 
				<span class='command_text'>f5-bigip-lbaas-agent</span> log files on their respective hosts. 	
				</p>
				
			</div>
			<div id='footer' name='footer'>
				&copy; 2014 F5 Networks
			</div>
		</section>

	</body>
</html>

