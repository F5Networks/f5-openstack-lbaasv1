User Guide==========Supported Neutron Release(s)----------------------------This version of the F5 OpenStack LBaaSv1 plugin supports the following OpenStack release(s):-  Icehouse-  Juno-  Kilo.The F5 OpenStack LBaaSv1 plugin does not encompass any alterations to the OpenStack community LBaaSv1 database schema; no additional database tables have been generated. The service plugin uses the stock community LoadBalancerPlugin class.Any issues with the community LoadBalancerPlugin class and the associated LBaaSv1 database model should be monitored and resolved through the OpenStack Neutron community.Supported Plugin Release------------------------The most recent, supported version of the F5 OpenStack LBaaSv1 plugin for the OpenStack releases noted above is v1.0.12. See `F5 OpenStack Releases and Support Matrix <http://f5networks.github.io/f5-openstack-docs/releases_and_versioning/>`_ for information about LBaaSv1 plugin, BIG-IP, and OpenStack release compatibility.Supported BIG-IP Releases-------------------------The F5 OpenStack LBaaSv1 plugin is compatible with BIG-IP 11.5 and greater.Overview--------The F5 OpenStack LBaaSv1 plugin allows you to orchestrate BIG-IP loadbalancing services -- including virtual IPs, pools, device servicegroups, and health monitoring -- in an OpenStack environment.The F5 LBaaSv1 agent translates 'OpenStack' to 'BIG-IP', so to speak,allowing you to provision BIG-IP LTM services in an OpenStackenvironment.The diagram below shows a sample OpenStack environment usingthe F5 plugin for OpenStack LBaaSv1. The LBaaSv1 agent communicates witha BIG-IP platform or Virtual Edition via iControl. The load balancingservice request is handled by the BIG-IP according to itsconfigurations; it can then connect, discover, and/or deploy to thecloud-based apps or vms in the OpenStack project network.    .. image:: media/openstack_lbaas_env_example.png        :width: 500        :alt: Sample OpenStack Environment with F5 LBaaSv1 PluginFigure 1. Sample OpenStack Environment with F5 LBaaSv1 PluginNeutron Networking Prerequisites--------------------------------Network Operation Mode``````````````````````The F5 OpenStack LBaaSv1 plugin supports two modes of network operation: `global routed mode <#global-routed-mode>`_ and `L2 adjacent mode <#l2-adjacent-mode>`_ (the default). The Neutron core provider requirements are different for each mode; the modes are described in detail later in this document. You can configure this in the ``L3 Segmentation Mode Settings`` section of the agent configuration file, as described `below <#configure-the-f5-lbaasv1-plugin>`_.ML2 Core Plugin```````````````Neutron is configured to use the ML2 core plugin by default. This configuration should appear in */etc/neutron/neutron.conf* as shown below.    .. code-block:: shell        core_plugin = neutron.plugins.ml2.plugin.Ml2PluginProvidernet Extension`````````````````````The Neutron providernet extension adds attributes to Neutron networks that define the L2 network settings required for device connectivity. The F5 LBaaSv1 agent uses providernet attributes to provision L2 connectivity on BIG-IP devices. If your Neutron network doesn't use the providernet extension, the agent will not be able to correctly provision L2 isolation and tenancy on your BIG-IP devices.**To see if your Neutron networks support the providernet extension:****IMPORTANT:** The \*starred\* attributes must be present for the agent tofunction properly.    .. code-block:: shell        # neutron net-show <network_name>        +-----------------------------+--------------------------------------+        | Field                       | Value                                |        +-----------------------------+--------------------------------------+        | admin_state_up              | True                                 |        | id                          | 07f92400-4bb6-4ebc-9b5e-eb8ffcd5b34c |        | name                        | Provider-VLAN-62                     |        | *provider:network_type*     | vlan                                 |        | *provider:physical_network* | ph-eth3                              |        | *provider:segmentation_id*  | 62                                   |        | router:external             | False                                |        | shared                      | True                                 |        | status                      | ACTIVE                               |        | subnets                     | a89aa39e-3a8e-4f2f-9b57-45aa052b87bf |        | tenant_id                   | 3aef8f59a43943359932300f634513b3     |        +-----------------------------+--------------------------------------+F5 OpenStack LBaaSv1 Plugin Components--------------------------------------The F5 OpenStack LBaaSv1 plugin is comprised of three packages:- f5-bigip-common- f5-oslbaasv1-agent- f5-oslbaasv1-driver.All are open source and accessible on GitHub at `F5Networks/f5-openstack-lbaasv1 <https://github.com/F5Networks/f5-openstack-lbaasv1>`__.F5 BIG-IP Common````````````````The f5-bigip-common package provides a means of communication with BIG-IPdevices via the iControl REST API.LBaaSv1 Plugin Agent and Driver```````````````````````````````The LBaaSv1 plugin is comprised of an agent package and a service providerdriver (hereafter referred to as 'driver') package. The driver should beinstalled on every host for which you want to provision BIG-IP services. Theagent must be installed on at least one host; it can be installed on multiplehosts, as described in further detail below.General LBaaSv1 Process Architecture------------------------------------When Neutron LBaaSv1 API calls are issued to your Neutron controller,the community LBaaSv1 plugin will attempt to use either a designatedservice provider driver or the default service provider driver toprovision LBaaSv1 resources.The F5 LBaaSv1 drivers, running within the Neutron controller process(es),utilize Neutron RPC messaging queues to issue provisioning tasks to specific F5agent processes. Upon starting and successfully communicating with configuredBIG-IP device API endpoints, each agent process registers its own specificnamed queue to receive tasks from one or multiple Neutron controllers.    .. figure:: media/plugin_agent_architecture.png        :alt: Plugin Agent ArchitectureFigure 2. F5 LBaaSv1 Plugin Agent ArchitectureThe F5 LBaaSv1 agents make callbacks to the F5 LBaaSv1 service providerdrivers to query additional Neutron network, port, and subnetinformation; allocate Neutron objects like fixed IP address for SelfIPand SNAT translation addresses; and report provisioning and pool status.These callback requests are placed on an RPC message queue which isprocessed by all listening F5 LBaaSv1 service provider drivers in around robin fashion. Since all Neutron controller processes are workingtransactionally off the same backend database, it does not matter whichof the available Neutron controller processes handle these callbackrequests.You must have at least one F5 plugin driver running in a Neutron controller.Likewise, you must have at least one running agent process. Implementingadditional drivers, one per Neutron controller, will scale outcommunications from agents to Neutron. Installing additional agents ondifferent hosts in the same BIG-IP environment (in other words, hosts that havethe same BIG-IP ``environment_prefix`` and iControl endpoint settings) adds scheduledredundancy to the provision process. Neutron LBaaSv1 binds pools to specificagents for the life of the pool. The redundancy allows other agents running inthe same environment to handle requests if the bound agent is not active.**NOTE:** If the bound agent is inactive, it's expected that it will be brought back online. If an agent is deleted, all pools bound to it should also be deleted. Run ``neutron lb-pool-list-on-agent <agent-id>`` to identify all pools associated with an agent.Two agents which have different iControl endpoint settings (in other words,agents that are provisioning different sets of BIG-IP devices) can not use thesame ``environment_prefix``. This would be interpreted by the scheduler as bothagents provisioning the same set of BIG-IP devices and could cause errors. When an LBaaSv1 API interface is invoked, the F5 LBaaSv1 service provider schedules agent tasks based upon an F5 agent's availability, as updated via the standard Neutron agent status messages.You can view all your running Neutron agent processes using the Neutron APIagent interfaces. Using the CLI client, use the ``neutron agent-list`` and``neutron agent-show`` commands.Deploying the F5 OpenStack LBaaSv1 Plugin-----------------------------------------Install LBaaSv1 Components``````````````````````````The most basic deployment consists of one F5 OpenStack LBaaSv1 driver and oneLBaaSv1 agent installed on the same Neutron controller. This is therecommended configuration for testing / POCs. Scale out and redundantinstallations can be added at any time. Alterations to the default installedservices to add redundancy and scale out are referenced later in this document.The F5 OpenStack LBaaSv1 plugin is distributed as a Debian or Red Hatinstallation package. To install the plugin, download the appropriate release package from`F5Networks/f5-openstack-lbaasv1 <https://github.com/F5Networks/f5-openstack-lbaasv1/>`_ toyour Neutron controller host(s), then install the components using the instructionsappropriate for your OS.Debian / Ubuntu~~~~~~~~~~~~~~~1. Install the F5 BIG-IP common libraries.   .. code-block:: shell      # dpkg -i build/deb_dist/f5-bigip-common_1.0.12_all.deb2. Install the plugin driver.   .. code-block:: shell      # dpkg -i build/deb_dist/f5-lbaas-driver_1.0.12_all.deb3. Install the plugin agent.   .. code-block:: shell      # dpkg -i build/deb_dist/f5-bigip-lbaas-agent_1.0.12_all.debRed Hat / CentOS~~~~~~~~~~~~~~~~1. Install the F5 BIG-IP common libraries.   .. code-block:: shell      # rpm -i build/el7/f5-bigip-common_1.0.12.noarch.el7.rpm2. Install the plugin driver.   .. code-block:: shell      # rpm -i build/el7/f5-lbaas-driver-1.0.12.noarch.el7.rpm3. Install the agent.   .. code-block:: shell      # rpm -i build/el7/f5-bigip-lbaas-agent-1.0.12.noarch.el7.rpmConfigure the F5 LBaaSv1 Plugin```````````````````````````````Before you begin~~~~~~~~~~~~~~~~In order to use the Neutron command sets, you need source a user filethat has admin permissions. (for example, ``source keystonerc_admin``).Configure the F5 LBaaSv1 Agent~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~The agent settings are found in */etc/neutron/f5-bigip-lbaas-agent.ini*. The file contains detailed explanations of each configuration option.**NOTE:** At minimum, you will need to edit the ``Device Settings``, ``Device Driver``, and ``L3 Segmentation Mode Settings`` sections of this file. Additional options are explained later in this document. The installation process automatically starts an agent process; after you configure the ``/etc/neutron/f5-oslbaasv1-agent.init`` file, `restart the agent process <#start/restart-the-agent>`_.Configure the Neutron Service~~~~~~~~~~~~~~~~~~~~~~~~~~~~~The Neutron service settings are found in */etc/neutron/neutron_lbaas.conf*. Edit the ``Default`` and ``Service Providers`` sections as shown below to tell Neutron to use the F5 LBaaSv1 service provider.**NOTE:** In the service providers section, the f5.os.lbaasv1driver entry will be present, but commented out. *Uncomment this line to identify the F5 plugin as the LBaaSv1 service provider.*  Add ':default' to the end of the line as shown below to set it as the default LBaaS service provider.    .. code-block:: shell        # vi /etc/neutron/neutron_lbaas.conf        [DEFAULT]        loadbalancer_plugin = neutron.services.loadbalancer.plugin.LoadBalancerPlugin        ...        [service providers]        service_provider = LOADBALANCER:F5:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriver:defaultSet the agent scheduler (Optional)~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~In the default section of your neutron.conf file, the ``f5_loadbalancer_pool_scheduler_driver`` variable can be set to an alternative agent scheduler. The default value for this setting, ``f5.oslbaasv1driver.drivers.agent_scheduler.TenantScheduler``, causes LBaaSv1 pools to be distributed within an environment with tenant affinity.**WARNING:** You should only provide an alternate scheduler if you have an alternate service placement requirement and your own scheduler.Restart the neutron service~~~~~~~~~~~~~~~~~~~~~~~~~~~   .. code-block:: shell    # service neutron-server restartRestart the http service~~~~~~~~~~~~~~~~~~~~~~~~    .. code-block:: shell        # service apache2 restart \\ Debian / Ubuntu        # service httpd restart   \\ Red Hat / CentOSStart/Restart the agent~~~~~~~~~~~~~~~~~~~~~~~The agent may start running automatically upon installation. Taking this step will start or restart the service, depending on the agent's current status.    .. code-block:: shell        # service f5-oslbaasv1-agent start**NOTE:** If you want to start with clean logs, you should remove the log file first:    .. code-block:: shell        # rm /var/log/neutron/f5-oslbaasv1-agent.logVerify the F5 LBaaSv1 Plugin is Active~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~To check the agent's status, run ``neutron agent-list``.    .. code-block:: shell        # neutron agent-list        +--------------------------------------+--------------------+----------------------------------------------+-------+----------------+---------------------------+        | id                                   | agent_type         | host                                         | alive | admin_state_up | binary                    |        +--------------------------------------+--------------------+----------------------------------------------+-------+----------------+---------------------------+        | 11b4c7ca-aaf9-4ac8-8b9f-2003e021cf23 | Metadata agent     | host-29                                      | :-)   | True           | neutron-metadata-agent    |        | 13c25ea9-ca58-4b69-af27-fb1ea8824f65 | L3 agent           | host-29                                      | :-)   | True           | neutron-l3-agent          |        | 4c71878e-ac49-4a60-81d3-af3793705460 | Open vSwitch agent | host-29                                      | :-)   | True           | neutron-openvswitch-agent |        | 4e9df1b2-4fb7-4d01-8758-ca139038b0c8 | Loadbalancer agent | host-29                                      | :-)   | True           | neutron-lbaas-agent       |        | 640c19de-4362-4c4e-88b1-650092e62169 | DHCP agent         | host-29                                      | :-)   | True           | neutron-dhcp-agent        |        | e4921123-000c-4172-8a79-72e8f0d357e2 | Loadbalancer agent | host-29:3eb793cb-fa51-549d-a15b-253ce5405fcf | :-)   | True           | f5-oslbaasv1-agent        |        +--------------------------------------+--------------------+----------------------------------------------+-------+----------------+---------------------------+To view more details, run ``neutron agent-show <agent-id>``.    .. code-block:: shell        # neutron agent-show e4921123-000c-4172-8a79-72e8f0d357e2        +---------------------+--------------------------------------------------------------------------+        | Field               | Value                                                                    |        +---------------------+--------------------------------------------------------------------------+        | admin_state_up      | True                                                                     |        | agent_type          | Loadbalancer agent                                                       |        | alive               | True                                                                     |        | binary              | f5-oslbaasv1-agent                                                       |        | configurations      | {                                                                        |        |                     |      "icontrol_endpoints": {                                             |        |                     |           "10.190.6.253": {                                              |        |                     |                "device_name": "host-10-20-0-4.int.lineratesystems.com",  |        |                     |                "platform": "Virtual Edition",                            |        |                     |                "version": "BIG-IP_v11.6.0",                              |        |                     |                "serial_number": "65d1af65-d236-407a-779a9e02c4d9"        |        |                     |           }                                                              |        |                     |      },                                                                  |        |                     |      "request_queue_depth": 0,                                           |        |                     |      "environment_prefix": "",                                           |        |                     |      "tunneling_ips": [],                                                |        |                     |      "common_networks": {},                                              |        |                     |      "services": 0,                                                      |        |                     |      "environment_capacity_score": 0,                                    |        |                     |      "tunnel_types": [                                                   |        |                     |           "gre",                                                         |        |                     |           "vlan",                                                        |        |                     |           "vxlan"                                                        |        |                     |      ],                                                                  |        |                     |      "environment_group_number": 1,                                      |        |                     |      "bridge_mappings": {                                                |        |                     |           "default": "1.1"                                               |        |                     |      },                                                                  |        |                     |      "global_routed_mode": false                                         |        |                     | }                                                                        |        | created_at          | 2016-02-12 23:13:40                                                      |        | description         |                                                                          |        | heartbeat_timestamp | 2016-02-16 17:35:11                                                      |        | host                | host-29:3eb793cb-fa51-549d-a15b-253ce5405fcf                             |        | id                  | e4921123-000c-4172-8a79-72e8f0d357e2                                     |        | started_at          | 2016-02-12 23:13:40                                                      |        | topic               | f5-lbaas-process-on-agent                                                |        +---------------------+--------------------------------------------------------------------------+If the ``f5-oslbaasv1-agent`` doesn't appear when you run ``neutron agent-list``, the agent is not running. The options below can be useful for troubleshooting: * Check the logs:    .. code-block:: shell        # less /var/log/neutron/f5-oslbaasv1-agent.log * Check the status of the f5-os-lbaasv1-agent service:    .. code-block:: shell        # systemctl status f5-oslbaasv1-agentMultiple Controllers and Agent Redundancy-----------------------------------------The F5 LBaaSv1 plugin driver runs within the Neutron controller. When the Neutron community LBaaS plugin loads thedriver, it creates a global messaging queue that will be used for all inboundcallbacks and status update requests from F5 LBaaSv1 agents. (To run multiple queues, see the`differentiated service <#differentiated-services-and-scale-out>`_ section below.)In an environment with multiple Neutron controllers the F5 drivers all listen to the samenamed message queue, providing controller redundancy and scale out. The drivers handle requests from the global queue in a round-robin fashion. All Neutron controllers must use the same Neutron database to avoid state problems with concurrently-running controller instances.    .. figure:: media/basic_agent_scheduled_redudancy.png        :alt: Basic Agent Scheduled Redundancy    Figure 3. Basic Agent Scheduled Redundancy**NOTE**: The agent service will expect to find an */etc/neutron/neutron.conf* file on its host; this file contains the configurations for Neutron messaging. To make sure the messaging settings match those of the controller, we recommend copying the /etc/neutron/neutron.conf from the controller to all additional hosts.If you choose to deploy multiple agents with the same BIG-IP ``environment_prefix``, each agent **must** run on a different host. Each agent will communicate with its configured iControl endpoint(s) to do the following: * Verify that the BIG-IP systems meet minimal requirements. * Create a specific named queue unique to itself for processing provisioning requests from service provider drivers. * Report as a valid F5 LBaaSv1 agent via the standard Neutron controller agent status queue.The agents continue to report their status to the agent queue on a periodic basis (every 10 seconds, bydefault; this can be configured in */etc/neutron/f5-bigip-lbaas-agent.ini*).When a Neutron controller receives a request for a new pool, the F5 LBaaSv1 driver invokes the Tenant scheduler. The schedulerqueries all active F5 agents and determines what, if any, existing pools are bound to each agent. If the driver locates an active agent that already has a bound pool for the same ``tenant_id`` as the newly-requested pool, the driver selects that agent. Otherwise, the driver selects an active agent at random. The request to create the pool service is sent to the selected agent's task queue. When the provisioning task is complete, the agent reports the outcome to the LBaaSv1 callback queue. The driver processes the agent's report and updates the Neutron database. The agent which handled the provisioning task is bound to the pool for the pool's lifetime (in other words, that agent will handle all tasks for that pool as long as the agent and/or pool are active). If a bound agent is inactive, the Tenant scheduler looks for other agents with the same ``environment_prefix`` as the bound agent. The scheduler assigns the task to the first active agent with a matching ``environment_prefix`` that it finds. The pool remains bound to the original (currently inactive) agent, with the expectation that the agent will eventually be brought back online.**NOTE:** If an agent is deleted, all pools bound to it should also be deleted. Run ``neutron lb-pool-list-on-agent <agent-id>`` to identify all pools associated with an agent.Differentiated Services and Scale Out-------------------------------------The F5 LBaaSv1 plugin supports deployments where multiple BIG-IP environments are required. In a differentiated service environment, each F5 driver will work as described above **with the exception** that each environment has its own messaging queue. The Tenant scheduler for each environment only considers agents within that environment. Configuring multiple environments with corresponding distinct ``neutron_lbaas`` service provider entries is the only way to allow a tenant to select its environment through the LBaaS API. The first section of */etc/neutron/f5-bigip-lbaas-agent.ini* provides information regarding configuration of multiple environments.To configure differentiated LBaaSv1 provisioning:1. Install the agent and driver on each host that requires LBaaSv1 provisioning.2. Assign the agent an environment-specific name in */etc/neutron/f5-bigip-lbaas-agent.ini*.3. Create a service provider entry for each agent in */etc/neutron/neutron_lbaas* that corresponds to the unique agent name you assigned.**WARNING:** A differentiated BIG-IP environment can not share anything. This precludes the use of vCMP for differentiated environments because vCMP guests share global VLAN IDs.    .. figure:: media/driver_multiple_environments.png        :alt: Installing the LBaaSv1 Driver in Multiple Environments    Figure 4. Installing the LBaaSv1 Driver in Multiple Environments    .. figure:: media/agent_multiple_environments.png        :alt: F5 LBaaSv1 Agents in Multiple Environments    Figure 5. F5 LBaaSv1 Agents in Multiple EnvironmentsDefault Environment Options```````````````````````````The F5 OpenStack LBaaSv1 plugin allows for the use of three default environment names - test, dev, and prod. As shown in the excerpt from */etc/neutron/f5-oslbaasv1-agent.ini* below, the service provider entries in */etc/neutron/neutron_lbaas* correspond to each agent's unique ``environment_prefix``.    .. code-block:: shell        # For a test environment:        #        # Set your agent's environment_prefix to 'test'        #        # and add the following line to your LBaaS service_provider config        # on the neutron server:        #        # service_provider = LOADBALANCER:TEST:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverTest        #        # For a dev environment:        #        # Set your agent's environment_prefix to 'dev'        #        # and add the following line to your LBaaS service_provider config        # on the neutron server:        #        # service_provider = LOADBALANCER:DEV:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverDev        #        # For a prod environment:        #        # Set your agent's environment_prefix to 'prod'        #        # and add the following line to your LBaaS service_provider config        # on the neutron server:        #        # service_provider = LOADBALANCER:PROD:f5.oslbaasv1driver.drivers.plugin_driver.F5PluginDriverProdAfter making changes to  */etc/neutron/f5-oslbaasv1-agent.ini* and */etc/neutron/neutron_lbaas*, restart the ``neutron-server`` process.    .. code-block:: shell        # service neutron-server restartRun ``neutron agent-list`` to view the list of active agents on your host to verify that the agent is up and running. If you do not see the ``f5-oslbaasv1-agent`` listed, you may need to restart the service.    .. code-block:: shell        # service f5-oslbaasv1-agent restartCustom Environments```````````````````You can use a driver-generating module to create custom environments. On each Neutron controller which will host your customenvironment, run the following command:    .. code-block:: shell        # python -m f5.oslbaasv1driver.utils.generate_env.py provider_name environment_prefixExample: Add the environment 'DFW1' using the following command:    .. code-block:: shell        # python -m f5.oslbaasv1driver.utils.generate_env.py DFW1 DFW1The command creates a driver class and a corresponding ``service_provider`` entry in */etc/neutron/neutron_lbaas*.    .. code-block:: shell        # service_provider = LOADBALANCER:DFW1:f5.oslbaasv1driver.drivers.plugin_driver_Dfw1.F5PluginDriverDfw1To activate your custom environment, remove the comment (`#`) from the beginning of the new ``service_provider`` line. Then, restart ``neutron-server``.Capacity-Based Scale Out Per Environment````````````````````````````````````````In a differentiated service environment you can configure multiple agents, each of which is associated with a distinct iControl endpoint (in other words, different BIG-IP devices). When grouping is specified within an environment, the service provider scheduler will consider the groupingalong with a reported ``environment_capacity_score``. Together, theagent grouping and the capacity score allow the scheduler to scale outa single environment across multiple BIG-IP device service groups.    .. figure:: media/env_group_scale_out.png        :alt: Environment Group Scale Out    Figure 6. Environment Group Scale OutTo enable environment grouping, edit the ``environment_group_number`` setting in */etc/neutron/f5-oslbaasv1-agent.ini* (excerpt shown below).    .. code-block:: shell        # When using service differentiated environments, the environment can be        # scaled out to multiple device service groups by providing a group number.        # Each agent associated with a specific device service group should have        # the same environment_group_number.        #        # environment_group_number = 1All agents in the same group should have the same ``environment_group_number`` setting.Each agent measures its BIG-IP devices' capacity. The agent will report a single ``environment_capacity_score`` for itsgroup every time it reports its agent status to the Neutron controller.The ``environment_capacity_score`` value is the highest capacity recorded on several collected statistics specified in the``capacity_policy`` setting in the agent configuration. The``capacity_policy`` setting is a dictionary, where the key is themetric name and the value is the max allowed value for that metric. Thescore is determined by dividing the metric collected by the max specifiedfor that metric in the ``capacity_policy`` setting. An acceptable reported ``environment_capacity_score`` is between zero (0) andone (1). **If an agent in the group reports an ``environment_capacity_score`` of one (1) or greater, the device is considered to be at capacity.**    .. code-block:: shell        # capacity_policy = throughput:1000000000, active_connections: 250000, route_domain_count: 512, tunnel_count: 2048**WARNING:** If you set the ``capacity_policy`` and all agents in all groups for an environment are at capacity, services will no longer be scheduled. When pools are created for an environment which has no capacity left, the pools will be placed in the error state.The following metrics implemented by the iControl driver can also be configured in */etc/neutron/f5-oslbaasv1-agent.ini*:    .. code-block:: shell        # throughput - total throughput in bps of the TMOS devices        # inbound_throughput - throughput in bps inbound to TMOS devices        # outbound_throughput - throughput in bps outbound from TMOS devices        # active_connections - number of concurrent active actions on a TMOS device        # tenant_count - number of tenants associated with a TMOS device        # node_count - number of nodes provisioned on a TMOS device        # route_domain_count - number of route domains on a TMOS device        # vlan_count - number of VLANs on a TMOS device        # tunnel_count - number of GRE and VxLAN overlay tunnels on a TMOS device        # ssltps - the current measured SSL TPS count on a TMOS device        # clientssl_profile_count - the number of clientside SSL profiles defined        #        # You can specify one or multiple metrics.When you create a new pool in an environment where multiple agent groups are configured, and the pool's ``tenant_id`` is not already associated with an agent group, the scheduler will attempt to assign the pool to the agent group which last reported the lowest ``environment_capacity_score``. If the pool's ``tenant_id`` is already associated with an agent group that is at capacity, the scheduler binds the pool to an agent in another group in the environment that is not at capacity.Running Multiple Agents on the Same Host````````````````````````````````````````**WARNING:** You should never run two agents *for the same environment* on the same host, as the hostname is used to help Neutron distinguish between agents. Multiple agent processes for *different environments* -- meaning each agent is associated with a different iControl endpoint -- can run on the same host.To configure multiple agent processes on the same host:1. Create a unique configuration file for each agent, using */etc/neutron/f5-oslbaasv1-agent.ini* as a template. Each   configuration file must have a unique iControl endpoint.2. Create additional upstart, init.d, or systemd service definitions for additional agents, using the default service definitions as a guide.   Each service should point to the appropriate configuration file (created in the previous step). The agent process uses Oslo   configuration. This means that typically the only thing that would change from the template service definitions would be the   ``--config-file`` and ``--log-file`` comand line arguments used to start the ``/usr/bin/f5-oslbaasv1-agent`` executable.3. Start each agent using the name of its unique upstart, init.d, or systemd service name.Supported  Network Topologies-----------------------------The F5 iControl agent driver supports the following network topologies with either BIG-IP hardware or virtual editions.Global routed mode``````````````````In global routed mode, all VIPs are assumed routable from clients andall Members are assumed routable from the BIG-IP devices themselves. AllL2 and L3 objects, including routes, must be pre-provisioned on the BIG-IPDevice Service Group prior to LBaaSv1 provisioning.    .. figure:: media/global_routed_mode.png        :alt: Global Routed ModeFigure 7. Global Routed Mode    .. code-block:: shell        +--------------------------------------+--------------------------------------+        | Topology                             | f5-oslbaasv1-agent.ini setting       |        +======================================+======================================+        | Global Routed mode                   | f5_global_routed_mode = True         |        +--------------------------------------+--------------------------------------+Global routed mode uses BIG-IP AutoMap SNAT for all VIPs. Because noexplicit SNAT pools are being defined, sufficient Self IP addressesshould be created to handle connection loads.**WARNING:** In global routed mode, because all access to and from theBIG-IP devices is assumed globally routed, there is no network segregationbetween tenant services on the BIG-IP devices themselves. Overlapping IPaddress spaces for tenant objects is likewise not available.L2 Adjacent Mode````````````````**L2 adjacent mode is the default mode.** In L2 adjacent mode, the F5 OpenStackLBaaSv1 agent attempts to provision L2 networks -- including VLANs andoverlay tunnels -- by associating a specific BIG-IP device with eachtenant network that has a VIP or pool member. VIP listeners are restricted totheir designated Neutron tenant network. L3 addresses associated withpool members are automatically allocated from Neutron subnets.L2 adjacent mode follows the `micro-segmentation <https://devcentral.f5.com/articles/microservices-versus-microsegmentation>`__ security model for gateways. Since each BIG-IP device is L2-adjacent to all tenant networks for which LBaaSv1 objects are provisioned, the traffic flows do notlogically pass through another L3 forwarding device. Instead, traffic flows arerestricted to direct L2 communication between the cloud network elementand the BIG-IP devices.    .. figure:: media/l2_adjacent_mode_topology.png        :alt: L2 Adjacent Mode TopologyFigure 8. L2 Adjacent Mode Topology    .. code-block:: shell        +--------------------------------------+--------------------------------------+        | Topology                             | f5-oslbaasv1-agent.ini setting       |        +======================================+======================================+        | L2 Adjacent mode                     | f5_global_routed_mode = False        |        +--------------------------------------+--------------------------------------+Because the agents manage the BIG-IP device associations for many tenantnetworks, L2 adjacent mode is a much more complex orchestration. Itdynamically allocates L3 addresses from Neutron tenant subnets for BIG-IPSelfIPs and SNAT translation addresses. These additional L3 addressesare allocated from the Neutron subnets associated with LBaaSv1 VIPs orMembers.One-Arm Mode````````````In one-arm mode, VIP and Members can be provisioned from the sameNeutron subnet.    .. figure:: media/one_arm.png        :alt: One-arm ModeFigure 9. One-arm Mode    .. code-block:: shell        +--------------------------------------+--------------------------------------+        | Topology                             | f5-oslbaasv1-agent.ini settings      |        +======================================+======================================+        | One-arm                              | f5_global_routed_mode = False        |        |                                      | f5_snat_mode = True                  |        |                                      |                                      |        |                                      | optional settings:                   |        |                                      | f5_snat_addresses_per_subnet = n     |        |                                      |                                      |        |                                      | where if n is 0, the virtual server  |        |                                      | will use AutoMap SNAT. If n is > 0,  |        |                                      | n number of SNAT addresses will be   |        |                                      | allocated from the Member subnet per |        |                                      | active traffic group.                |        +--------------------------------------+--------------------------------------+Multiple-Arm mode`````````````````In multiple-arm mode, VIP and Members are provisioned from differentNeutron subnets.    .. figure:: media/multiarm_snat.png        :alt: Multiple-arm ModeFigure 10. Multiple-arm Mode    .. code-block:: shell        +--------------------------------------+--------------------------------------+        | Topology                             | f5-oslbaasv1-agent.ini setting       |        +======================================+======================================+        | Multiple-arm                         | f5_global_routed_mode = False        |        |                                      | f5_snat_mode = True                  |        |                                      |                                      |        |                                      | optional settings:                   |        |                                      | f5_snat_addresses_per_subnet = n     |        |                                      |                                      |        |                                      | where if n is 0, the virtual server  |        |                                      | will use AutoMap SNAT. If n is > 0,  |        |                                      | n number of SNAT addresses will be   |        |                                      | allocated from the Member subnet per |        |                                      | active traffic group.                |        +--------------------------------------+--------------------------------------+Gateway Routed Mode```````````````````In gateway routed mode, attemps will be made to create a default gatewayforwarding service on the BIG-IP Device Service Group for Member Neutronsubnets.    .. figure:: media/routed_mode.png        :alt: Gateway Routed ModeFigure 11. Gateway Routed Mode    .. code-block:: shell        +--------------------------------------+--------------------------------------+        | Topology                             | f5-oslbaasv1-agent.ini setting       |        +======================================+======================================+        | Gateway routed mode                  | f5_global_routed_mode = False        |        |                                      | f5_snat_mode = False                 |        |                                      |                                      |        +--------------------------------------+--------------------------------------+For the Neutron network topologies requiring dynamic L2 and L3provisioning of the BIG-IP devices -- **which includes all network topologiesexcept global routed mode** -- the F5 LBaaSv1 iControl driver supports the following:-  Provider VLANs - VLANs defined by the admin tenant and shared with other tenants-  Tenant VLANs - VLANs defined by the admin tenant *for* other tenants, or defined   by the tenants themselves-  Tenant GRE Tunnels - GRE networks defined by the tenant-  Tenant VxLAN Tunnels - VxLAN networks defined by the tenantVLANs`````For VLAN connectivity, the F5 BIG-IP devices use a mapping between theNeutron ``network provider:physical_network`` attribute and TMMinterface names. This is analogous to the Open vSwitch agents mappingbetween the Neutron ``network provider:physical_network`` and theinterface bridge name. The mapping is created in */etc/neutron/f5-oslbaasv1-agent.ini*, using the``f5_external_physical_mappings`` setting. The name of the ``provider:physical_network`` entries can be added to a comma separatedlist with mappings to the TMM interface or LAG trunk name, and a booleanattribute to specify if 802.1q tagging will be applied.Example: This configuration maps the ``provider:physical_network`` containing 'ph-eth3' to TMMinterface 1.1 with 802.1q tagging.    .. code-block:: shell        f5_external_physical_mappings = ph-eth3:1.1:TrueA default mapping should be included for cases where the ``provider:physical_network`` does not match any configuration settings.A default mapping simply uses the word default instead of a known``provider:physical_network`` attribute.Example: The configuration below includes the previously illustrated ``ph-eth3`` map, a default map, and LAG trunkmapping.    .. code-block:: shell        f5_external_physical_mappings = default:1.1:True, ph-eth3:1.1:True, ph-eth4:lag-trunk-1:True**WARNING:** The default Open vSwitch Neutron networking does notsupport VLAN tagging by guest instances. Each guest interface is treatedas an access port and all VLAN tags will be stripped before frames reachthe physical network infrastructure. To allow a BIG-IP VE guest tofunction in L2 Adjacent mode using VLANs as your tenant network type, thesoftware networking infrastructure which strips VLAN tags from framesmust be bypassed. You can bypass the software bridge using the ``ip``, ``brctl``, and ``ovs-vsctl`` commands on the compute nodeafter the BIG-IP VE guest instances have been created. This process is **not** automated by any Neutron agent. This requirement only appliesto BIG-IP VE when running as a Nova guest instance.    .. figure:: media/VE_Multitenant_VLAN_bypass.png        :alt: VE Multi-tenant VLAN BypassFigure 12. VE Multi-tenant VLAN BypassTunnels```````For GRE and VxLAN tunnels, the F5 BIG-IP devices expect to communicatewith Open vSwitch VTEPs. The VTEP addresses for Open vSwitch VTEPs arelearned from their registered Neutron agent configuration's ``tunneling_ip`` attribute.For example:    .. code-block:: shell        # neutron agent-show 034bddd0-0ac3-457a-9e2c-ed456dc2ad53        +---------------------+--------------------------------------+        | Field               | Value                                |        +---------------------+--------------------------------------+        | admin_state_up      | True                                 |        | agent_type          | Open vSwitch agent                   |        | alive               | True                                 |        | binary              | neutron-openvswitch-agent            |        | configurations      | {                                    |        |                     |      "tunnel_types": [               |        |                     |           "gre"                      |        |                     |      ],                              |        |                     |      "tunneling_ip": "10.1.0.35",    |        |                     |      "bridge_mappings": {            |        |                     |           "ph-eth3": "br-eth3"       |        |                     |      },                              |        |                     |      "l2_population": true,          |        |                     |      "devices": 4                    |        |                     | }                                    |        | created_at          | 2013-11-15 05:00:23                  |        | description         |                                      |        | heartbeat_timestamp | 2014-04-22 16:58:21                  |        | host                | sea-osp-cmp-001                      |        | id                  | 034bddd0-0ac3-457a-9e2c-ed456dc2ad53 |        | started_at          | 2014-04-17 22:39:30                  |        | topic               | N/A                                  |        +---------------------+--------------------------------------+The F5 LBaaSv1 agent supports the ML2 L2 population service in that overlay tunnels for Member IP access are only built to Open vSwitch agents hosting Members. When using the ML2 population service, you can also elect to use static ARP entries for BIG-IP devices to avoid flooding. This setting is found in */etc/neutron/f5-oslbaasv1-agent.ini*.    .. code-block:: shell        # Static ARP population for members on tunnel networks        #        # This is a boolean True or False value which specifies        # that if a Pool Member IP address is associated with a gre        # or vxlan tunnel network, in addition to a tunnel fdb        # record being added, that a static arp entry will be created to        # avoid the need to learn the member's MAC address via flooding.        #        f5_populate_static_arp = TrueThe necessary ML2 port binding extensions and segmentation model are defined by default with the community ML2 coreplugin and Open vSwitch agents on the compute nodes.When VIPs are placed on tenant overlay networks, the F5 LBaaSv1 agentsends tunnel update RPC messages to the Open vSwitch agents to inform them of BIG-IP device VTEPs. This allows tenant guest virtualmachines or network node services to interact with the BIG-IP-provisioned VIPs across overlay networks.BIG-IP VTEP addresses should be added to the associated agent's config file (*/etc/neutron/f5-oslbaasv1-agent.ini*).    .. code-block:: shell        # Device Tunneling (VTEP) selfips        #        # This is a single entry or comma separated list of cidr (h/m) format        # selfip addresses, one per BIG-IP device, to use for VTEP addresses.        #        # If no gre or vxlan tunneling is required, these settings should be        # commented out or set to None.        #        #f5_vtep_folder = 'Common'        #f5_vtep_selfip_name = 'vtep'Run ``neutron agent-show <agent-id>`` to view/verify the VTEP configurations. The VTEP addresses are listed as ``tunneling_ips``.    .. code-block:: shell        # neutron agent-show 014ada1a-91ab-4408-8a81-7be6c4ea8113        +---------------------+-----------------------------------------------------------------------+        | Field               | Value                                                                 |        +---------------------+-----------------------------------------------------------------------+        | admin_state_up      | True                                                                  |        | agent_type          | Loadbalancer agent                                                    |        | alive               | True                                                                  |        | binary              | f5-bigip-lbaas-agent                                                  |        | configurations      | {                                                                     |        |                     |      "icontrol_endpoints": {                                          |        |                     |           "10.0.64.165": {                                            |        |                     |                "device_name": "host-10-0-64-165.openstack.f5se.com",  |        |                     |                "platform": "Virtual Edition",                         |        |                     |                "version": "BIG-IP_v11.6.0",                           |        |                     |                "serial_number": "b720f143-a632-464c-4db92773f2a0"     |        |                     |           },                                                          |        |                     |           "10.0.64.164": {                                            |        |                     |                "device_name": "host-10-0-64-164.openstack.f5se.com",  |        |                     |                "platform": "Virtual Edition",                         |        |                     |                "version": "BIG-IP_v11.6.0",                           |        |                     |                "serial_number": "e1b1f439-72c3-5240-4358bbc45dff"     |        |                     |           }                                                           |        |                     |      },                                                               |        |                     |      "request_queue_depth": 0,                                        |        |                     |      "environment_prefix": "dev",                                     |        |                     |      "tunneling_ips":                                                 |        |                     |           "10.0.63.126",                                              |        |                     |           "10.0.63.125"                                               |        |                     |      ],                                                               |        |                     |      "common_networks": {},                                           |        |                     |      "services": 0,                                                   |        |                     |      "environment_capacity_score": 0,                                 |        |                     |      "tunnel_types": [                                                |        |                     |           "gre"                                                       |        |                     |      ],                                                               |        |                     |      "environment_group_number": 1,                                   |        |                     |      "bridge_mappings": {                                             |        |                     |           "default": "1.3"                                            |        |                     |      },                                                               |        |                     |      "global_routed_mode": false                                      |        |                     | }                                                                     |        | created_at          | 2015-08-19 13:08:15                                                   |        | description         |                                                                       |        | heartbeat_timestamp | 2015-08-20 15:19:15                                                   |        | host                | sea-osp-ctl-001:f5acc0d3-24d6-5c64-bc75-866dd26310a4                  |        | id                  | 014ada1a-91ab-4408-8a81-7be6c4ea8113                                  |        | started_at          | 2015-08-19 17:30:44                                                   |        | topic               | f5-lbaas-process-on-agent                                             |        +---------------------+-----------------------------------------------------------------------+OpenStack and BIG-IP Multinenancy---------------------------------By default, all BIG-IP objects are created in administrative partitionsassociated with the OpenStack ``tenant_id`` for the Pool. If the */etc/neutron/f5-oslbaasv1-agent.ini* setting for ``use_namespaces``is set to ``True``, and it is not configured for global routed mode, a BIG-IP route domain is created for each tenant, providingsegmentation for IP address spaces between tenants. If an associated Neutron network for a VIP or Member is shown as ``shared=True``, and the F5 LBaaSv1 agent is not in global routed mode, all associated L2 and L3 objects are created in the /Common administrative partition and associated with route domain 0 (zero) on all BIG-IP devices.    .. figure:: media/tenancy_mapping.png        :alt: BIG-IP Multi-tenancyFigure 13. BIG-IP Multi-tenancyBIG-IP High Availability Modes------------------------------The F5 iControl agent driver supports:-  Standalone - No High Availability-  Pair mode - Active / Standby BIG-IP devices-  ScaleN mode - Multiple Active BIG-IP devices, up to 4 (four) devicesThese options can be configured in the ``Device Settings`` section of */etc/neutron/f5-oslbaasv1-agent.ini*.Troubleshooting---------------To troubleshoot problems with the F5 LBaaSv1 driver or an agent process, set the global Neutron setting and agent process ``debug`` setting to ``True``. Extensive logging will then appear in the neutron-server and f5-oslbaasv1-agent log files on their respective hosts.